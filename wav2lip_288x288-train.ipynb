{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9f408c",
   "metadata": {
    "papermill": {
     "duration": 0.040765,
     "end_time": "2021-08-02T15:03:04.783507",
     "exception": false,
     "start_time": "2021-08-02T15:03:04.742742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 之前代码有问题，以最新版为准"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e6943",
   "metadata": {
    "papermill": {
     "duration": 0.039069,
     "end_time": "2021-08-02T15:03:04.861534",
     "exception": false,
     "start_time": "2021-08-02T15:03:04.822465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 任务\n",
    "\n",
    "本次实践涉及到对Wav2Lip模型的，以及相关代码实现。总体上分为以下几个部分：\n",
    "1. 环境的配置\n",
    "2. 数据集准备及预处理\n",
    "3. 模型的训练\n",
    "4. 模型的推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a35e39",
   "metadata": {
    "papermill": {
     "duration": 0.03953,
     "end_time": "2021-08-02T15:03:04.941630",
     "exception": false,
     "start_time": "2021-08-02T15:03:04.902100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Wav2Lip\n",
    "**[Wav2Lip](https://arxiv.org/pdf/2008.10010.pdf)** 是一种基于对抗生成网络的由语音驱动的人脸说话视频生成模型。如下图所示，Wav2Lip的网络模型总体上分成三块：生成器、判别器和一个预训练好的Lip-Sync Expert组成。网络的输入有2个：任意的一段视频和一段语音，输出为一段唇音同步的视频。生成器是基于encoder-decoder的网络结构，分别利用2个encoder: speech encoder, identity encoder去对输入的语音和视频人脸进行编码，并将二者的编码结果进行拼接，送入到 face decoder 中进行解码得到输出的视频帧。判别器Visual Quality Discriminator对生成结果的质量进行规范，提高生成视频的清晰度。为了更好的保证生成结果的唇音同步性，Wav2Lip引入了一个预预训练的唇音同步判别模型 Pre-trained Lip-sync Expert，作为衡量生成结果的唇音同步性的额外损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34513e2",
   "metadata": {
    "papermill": {
     "duration": 0.037914,
     "end_time": "2021-08-02T15:03:05.018513",
     "exception": false,
     "start_time": "2021-08-02T15:03:04.980599",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Lip-Sync Expert\n",
    "Lip-sync Expert基于 **[SyncNet](https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16a/)**，是一种用来判别语音和视频是否同步的网络模型。如下图所示，SyncNet的输入也是两种：语音特征MFCC和嘴唇的视频帧，利用两个基于卷积神经网络的Encoder分别对输入的语音和视频帧进行降纬和特征提取，将二者的特征都映射到同一个纬度空间中去，最后利用contrastive loss对唇音同步性进行衡量，结果的值越大代表越不同步，结果值越小则代表越同步。在Wav2Lip模型中，进一步改进了SyncNet的网络结构：网络更深；加入了残差网络结构；输入的语音特征被替换成了mel-spectrogram特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461cfed",
   "metadata": {
    "papermill": {
     "duration": 0.07419,
     "end_time": "2021-08-02T15:03:05.158517",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.084327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. 环境的配置\n",
    "- `建议准备一台有显卡的linux系统电脑，或者可以选择使用第三方云服务器（Google Colab）` \n",
    "- `Python 3.6 或者更高版本` \n",
    "- ffmpeg: `sudo apt-get install ffmpeg`\n",
    "- 必要的python包的安装，所需要的库名称都已经包含在`requirements.txt`文件中，可以使用 `pip install -r requirements.txt`一次性安装. \n",
    "- 在本实验中利用到了人脸检测的相关技术，需要下载人脸检测预训练模型：Face detection [pre-trained model](https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth) 并移动到 `face_detection/detection/sfd/s3fd.pth`文件夹下. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1514d8",
   "metadata": {
    "papermill": {
     "duration": 0.081877,
     "end_time": "2021-08-02T15:03:05.314858",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.232981",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting librosa>=0.7.0 (from -r requirements.txt (line 1))\n",
      "  Downloading librosa-0.10.0.post2-py3-none-any.whl (253 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.24.3)\n",
      "Collecting opencv-contrib-python>=4.2.0.34 (from -r requirements.txt (line 3))\n",
      "  Downloading opencv_contrib_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: opencv-python>=4.1.0.25 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.6.0.66)\n",
      "Requirement already satisfied: torch>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (2.0.1)\n",
      "Requirement already satisfied: torchvision>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.15.2)\n",
      "Requirement already satisfied: tqdm>=4.45.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (4.65.0)\n",
      "Requirement already satisfied: numba>=0.48 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.57.0)\n",
      "Collecting audioread>=2.1.9 (from librosa>=0.7.0->-r requirements.txt (line 1))\n",
      "  Downloading audioread-3.0.0.tar.gz (377 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa>=0.7.0->-r requirements.txt (line 1)) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa>=0.7.0->-r requirements.txt (line 1)) (1.2.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa>=0.7.0->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa>=0.7.0->-r requirements.txt (line 1)) (5.1.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa>=0.7.0->-r requirements.txt (line 1))\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_17_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pooch<1.7,>=1.0 (from librosa>=0.7.0->-r requirements.txt (line 1))\n",
      "  Downloading pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting soxr>=0.3.2 (from librosa>=0.7.0->-r requirements.txt (line 1))\n",
      "  Downloading soxr-0.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa>=0.7.0->-r requirements.txt (line 1)) (4.5.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from librosa>=0.7.0->-r requirements.txt (line 1)) (0.3)\n",
      "Collecting msgpack>=1.0 (from librosa>=0.7.0->-r requirements.txt (line 1))\n",
      "  Downloading msgpack-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.8/316.8 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.1.0->-r requirements.txt (line 5)) (3.12.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.1.0->-r requirements.txt (line 5)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.1.0->-r requirements.txt (line 5)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.1.0->-r requirements.txt (line 5)) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.1.0->-r requirements.txt (line 5)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.1.0->-r requirements.txt (line 5)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.1.0->-r requirements.txt (line 5)) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.1.0->-r requirements.txt (line 5)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.1.0->-r requirements.txt (line 5)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.1.0->-r requirements.txt (line 5)) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.1.0->-r requirements.txt (line 5)) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.1.0->-r requirements.txt (line 5)) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.1.0->-r requirements.txt (line 5)) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.1.0->-r requirements.txt (line 5)) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.1.0->-r requirements.txt (line 5)) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.1.0->-r requirements.txt (line 5)) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.1.0->-r requirements.txt (line 5)) (67.7.2)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.1.0->-r requirements.txt (line 5)) (0.40.0)\n",
      "Requirement already satisfied: cmake in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.1.0->-r requirements.txt (line 5)) (3.27.0)\n",
      "Requirement already satisfied: lit in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.1.0->-r requirements.txt (line 5)) (16.0.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision>=0.3.0->-r requirements.txt (line 6)) (2.29.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision>=0.3.0->-r requirements.txt (line 6)) (9.5.0)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from numba>=0.48->-r requirements.txt (line 8)) (0.40.0)\n",
      "Collecting appdirs>=1.3.0 (from pooch<1.7,>=1.0->librosa>=0.7.0->-r requirements.txt (line 1))\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pooch<1.7,>=1.0->librosa>=0.7.0->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torchvision>=0.3.0->-r requirements.txt (line 6)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torchvision>=0.3.0->-r requirements.txt (line 6)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torchvision>=0.3.0->-r requirements.txt (line 6)) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torchvision>=0.3.0->-r requirements.txt (line 6)) (2023.5.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa>=0.7.0->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa>=0.7.0->-r requirements.txt (line 1)) (1.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.1.0->-r requirements.txt (line 5)) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.1.0->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.7.0->-r requirements.txt (line 1)) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->pooch<1.7,>=1.0->librosa>=0.7.0->-r requirements.txt (line 1)) (3.0.9)\n",
      "Building wheels for collected packages: audioread\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23706 sha256=81c55b6a5e9814e2e92a1b48843e19968ce5b1dab32082a558541ce56abb905f\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/da/4b/39/c5f6c4ee93b43281dda4dab5ac5f2bdf9d11074d427493cd55\n",
      "Successfully built audioread\n",
      "Installing collected packages: msgpack, appdirs, soxr, opencv-contrib-python, audioread, soundfile, pooch, librosa\n",
      "  Attempting uninstall: pooch\n",
      "    Found existing installation: pooch 1.7.0\n",
      "    Uninstalling pooch-1.7.0:\n",
      "      Successfully uninstalled pooch-1.7.0\n",
      "Successfully installed appdirs-1.4.4 audioread-3.0.0 librosa-0.10.0.post2 msgpack-1.0.5 opencv-contrib-python-4.8.0.74 pooch-1.6.0 soundfile-0.12.1 soxr-0.3.5\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c15ff",
   "metadata": {
    "papermill": {
     "duration": 0.065854,
     "end_time": "2021-08-02T15:03:05.446911",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.381057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. 数据集的准备及预处理\n",
    "\n",
    "**LRS2 数据集的下载**  \n",
    "实验所需要的数据集下载地址为：<a href=\"http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html\">LRS2 dataset</a>，下载该数据集需要获得BBC的许可，需要发送申请邮件以获取下载密钥，具体操作详见网页中的指示。下载完成后对数据集进行解压到本目录的`mvlrs_v1/`文件夹下，并将LRS2中的文件列表文件`train.txt, val.txt, test.txt` 移动到`filelists/`文件夹下，最终得到的数据集目录结构如下所示。\n",
    "```\n",
    "data_root (mvlrs_v1)\n",
    "├── main, pretrain (我们只使用main文件夹下的数据)\n",
    "|\t├── 文件夹列表\n",
    "|\t│   ├── 5位以.mp4结尾的视频ID\n",
    "```\n",
    "**数据集预处理**\n",
    "数据集中大多数视频都是包含人的半身或者全身的画面，而我们的模型只需要人脸这一小部分。所以在预处理阶段，我们要对每一个视频进行分帧操作，提取视频的每一帧，之后使用`face detection`工具包对人脸位置进行定位并裁减，只保留人脸的图片帧。同时，我们也需要将每一个视频中的语音分离出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "931aaa3d-6eab-44f9-9cc0-d6e17fe80c26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-31 01:37:25--  https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\n",
      "Resolving www.adrianbulat.com (www.adrianbulat.com)... 45.136.29.207\n",
      "Connecting to www.adrianbulat.com (www.adrianbulat.com)|45.136.29.207|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 89843225 (86M) [application/octet-stream]\n",
      "Saving to: ‘face_detection/detection/sfd/s3fd.pth’\n",
      "\n",
      "100%[======================================>] 89,843,225  21.9MB/s   in 4.5s   \n",
      "\n",
      "2023-07-31 01:37:31 (19.1 MB/s) - ‘face_detection/detection/sfd/s3fd.pth’ saved [89843225/89843225]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" -O face_detection/detection/sfd/s3fd.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73bccdb9-1b21-48d4-a983-6f2add90b315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf ../LSR2/demo\n",
    "!mkdir -p ../LSR2/demo\n",
    "!cp -r ../LSR2/main/553* ../LSR2/demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b53dcfde",
   "metadata": {
    "papermill": {
     "duration": 0.076313,
     "end_time": "2021-08-02T15:03:05.590493",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.514180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Started processing for ../LSR2/demo with 1 GPUs\n",
      "100%|█████████████████████████████████████████| 432/432 [10:17<00:00,  1.43s/it]\n",
      "Dumping audios...\n",
      "100%|█████████████████████████████████████████| 432/432 [00:15<00:00, 28.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# !rm -rf ../LSR2/lrs2_preprocessed_288x288\n",
    "# !python preprocess.py --data_root \"../LSR2/main\" --preprocessed_root \"../LSR2/lrs2_preprocessed_288x288\" --batch_size 128\n",
    "!rm -rf ../LSR2/lrs2_preprocessed_288x288-demo\n",
    "!python preprocess.py --data_root \"../LSR2/demo\" --preprocessed_root \"../LSR2/lrs2_preprocessed_288x288-demo\" --batch_size 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e70f29",
   "metadata": {
    "papermill": {
     "duration": 0.057029,
     "end_time": "2021-08-02T15:03:05.717822",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.660793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "预处理后的`lrs2_preprocessed/`文件夹下的目录结构如下\n",
    "```\n",
    "preprocessed_root (lrs2_preprocessed)\n",
    "├── 文件夹列表\n",
    "|\t├── 五位的视频ID\n",
    "|\t│   ├── *.jpg\n",
    "|\t│   ├── audio.wav\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a91f05-71fd-47f1-9488-df60a5653f0e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "codeformer_cmd = 'cd ../CodeFormer && python inference_codeformer.py --bg_upsampler realesrgan --face_upsample -w 1.0 --input_path {} --output_path {}'\n",
    "preprocessed_root = \"../LSR2/lrs2_preprocessed_288x288-demo\"\n",
    "sub_dirs = os.listdir(preprocessed_root)\n",
    "for sub_dir in tqdm(sub_dirs):\n",
    "    video_dirs = os.listdir(os.path.join(preprocessed_root, sub_dir))\n",
    "    for video_dir in video_dirs:\n",
    "        video_dir = os.path.join(preprocessed_root, sub_dir, video_dir)\n",
    "        # print(video_dir)\n",
    "        # print(codeformer_cmd.format(video_dir, video_dir))\n",
    "        os.system(codeformer_cmd.format(video_dir, video_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff5bb63-382e-4670-bd8e-aa6d7702c34d",
   "metadata": {},
   "source": [
    "获取对应的文件列表并更新到filelists/train.txt和filelists/eval.txt。只保存对应的视频名称即可。代码可以参考，对视频样本重命名并生成对应的命名列表，此处视频文件数量过少<2，会报错："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c52dd89-d180-4fac-900b-219cd4147b98",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5535864093654496929', '5538635636050605931', '5537751731781090844', '5537369050195015499', '5537514649586349811', '5539474443163516678', '5537522380527482610', '5536266102593401990', '5536915501648559593', '5539826200985059108', '5539535002202392187', '5539702505926936192', '5535423430009926848', '5537693749722594824', '5536968329746298779', '5539741160632598296', '5537885734760724252', '5535496873950688380', '5535415699068794046', '5537893465701857051', '5536760882825901738', '5536745420943636139', '5537143564411975377', '5539444807889172133', '5536038039829982468', '5536876846942893978']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from glob import glob\n",
    "import shutil,os\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "\n",
    "# 去除名字的特殊符号，统一序号视频文件命名\n",
    " \n",
    "# def original_video_name_format():\n",
    "#     base_path = \"../LSR2/main\"\n",
    "#     result = list(glob(\"{}/*\".format(base_path),recursive=False))\n",
    "#     file_num = 0\n",
    "#     result_list = []\n",
    " \n",
    "#     for each in result:\n",
    "#         file_num +=1\n",
    "#         new_position =\"{0}{1}\".format( int(time.time()),file_num)\n",
    "#         result_list.append(new_position)\n",
    "#         shutil.move(each, os.path.join(base_path,new_position+\".mp4\"))\n",
    "#         pass\n",
    "\n",
    "def trained_data_name_format():\n",
    "    base_path = preprocessed_root\n",
    "    # result = list(glob(\"{}/*\".format(base_path)))\n",
    "    result = os.listdir(base_path)\n",
    "    print(result)\n",
    "    result_list = []\n",
    "    for i,dirpath in enumerate(result):\n",
    "        # shutil.move(dirpath,\"{0}/{1}\".format(base_path,i))\n",
    "        # result_list.append(str(i))\n",
    "        # print('dirpath:', dirpath)\n",
    "        result_list.append(dirpath)\n",
    "    if len(result_list)<14:\n",
    "        test_result=val_result=train_result=result_list\n",
    "    else:\n",
    "        train_result,test_result = train_test_split(result_list,test_size=0.15, random_state=42)\n",
    "        test_result, val_result = train_test_split(test_result, test_size=0.5, random_state=42)\n",
    " \n",
    "    for file_name,dataset in zip((\"train.txt\",\"test.txt\",\"val.txt\"),(train_result,test_result,val_result)):\n",
    "        with open(os.path.join(\"filelists\",file_name),'w',encoding='utf-8') as fi:\n",
    "            for dataset_i in dataset:\n",
    "                # print('dataset_i:', dataset_i)\n",
    "                video_result = os.listdir(os.path.join(base_path, dataset_i))\n",
    "                # print('video_result:', video_result)\n",
    "                video_result = [dataset_i+'/'+video+'/final_results' for video in video_result]\n",
    "                fi.write(\"\\n\".join(video_result))\n",
    "                fi.write(\"\\n\")\n",
    " \n",
    "    # print(\"\\n\".join(result_list))\n",
    "\n",
    "trained_data_name_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f6d2a7-7f3e-40ea-96c4-996aaee42a32",
   "metadata": {},
   "source": [
    "Training the expert discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0bb64-29af-486d-b14e-5eecf0be8c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python color_syncnet_train.py --data_root ../LSR2/lrs2_preprocessed_288x288-demo/ --checkpoint_dir ./savedmodel \n",
    "# --checkpoint_path ./checkpoints/lipsync_expert.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b3dc53-722f-428e-8305-7b9d42ae082a",
   "metadata": {},
   "source": [
    "执行如下命令，开始训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b989d6e-be14-47fc-9f10-2226f02bc4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python wav2lip_train.py --data_root ../LSR2/lrs2_preprocessed --checkpoint_dir ./savedmodel --syncnet_checkpoint_path ./checkpoints/lipsync_expert.pth --checkpoint_path ./checkpoints/wav2lip.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764fda7-3c76-45d8-be8f-391ad409a361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python hq_wav2lip_train.py --data_root ../LSR2/lrs2_preprocessed --checkpoint_dir ./savedmodel --syncnet_checkpoint_path ./checkpoints/lipsync_expert.pth --checkpoint_path ./checkpoints/wav2lip.pth --disc_checkpoint_path ./checkpoints/visual_quality_disc.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04656c-3efc-4ddc-a4db-1f328b5cf457",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python wloss_hq_wav2lip_train.py --data_root ../LSR2/lrs2_preprocessed_288x288-demo/ --checkpoint_dir ./savedmodel --syncnet_checkpoint_path ./savedmodel/checkpoint_step000005500.pth \n",
    "# --checkpoint_path ./checkpoints/wav2lip.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06f8d8df-60a8-42e7-8827-b7eabf39cc42",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference.\n",
      "Reading video frames...\n",
      "Number of frames available for inference: 345\n",
      "(80, 936)\n",
      "Length of mel chunks: 349\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██                                          | 1/22 [00:15<05:17, 15.14s/it]\u001b[A\n",
      "  9%|████                                        | 2/22 [00:16<02:21,  7.08s/it]\u001b[A\n",
      " 14%|██████                                      | 3/22 [00:17<01:24,  4.44s/it]\u001b[A\n",
      " 18%|████████                                    | 4/22 [00:19<00:57,  3.20s/it]\u001b[A\n",
      " 23%|██████████                                  | 5/22 [00:20<00:42,  2.52s/it]\u001b[A\n",
      " 27%|████████████                                | 6/22 [00:21<00:33,  2.12s/it]\u001b[A\n",
      " 32%|██████████████                              | 7/22 [00:23<00:27,  1.85s/it]\u001b[A\n",
      " 36%|████████████████                            | 8/22 [00:24<00:23,  1.68s/it]\u001b[A\n",
      " 41%|██████████████████                          | 9/22 [00:25<00:20,  1.56s/it]\u001b[A\n",
      " 45%|███████████████████▌                       | 10/22 [00:27<00:17,  1.49s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 11/22 [00:28<00:15,  1.42s/it]\u001b[A\n",
      " 55%|███████████████████████▍                   | 12/22 [00:29<00:13,  1.39s/it]\u001b[A\n",
      " 59%|█████████████████████████▍                 | 13/22 [00:31<00:12,  1.39s/it]\u001b[A\n",
      " 64%|███████████████████████████▎               | 14/22 [00:32<00:11,  1.42s/it]\u001b[A\n",
      " 68%|█████████████████████████████▎             | 15/22 [00:33<00:09,  1.41s/it]\u001b[A\n",
      " 73%|███████████████████████████████▎           | 16/22 [00:35<00:08,  1.38s/it]\u001b[A\n",
      " 77%|█████████████████████████████████▏         | 17/22 [00:36<00:06,  1.37s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▏       | 18/22 [00:37<00:05,  1.37s/it]\u001b[A\n",
      " 86%|█████████████████████████████████████▏     | 19/22 [00:39<00:04,  1.36s/it]\u001b[A\n",
      " 91%|███████████████████████████████████████    | 20/22 [00:40<00:02,  1.34s/it]\u001b[A\n",
      " 95%|█████████████████████████████████████████  | 21/22 [00:41<00:01,  1.33s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 22/22 [00:50<00:00,  2.30s/it]\u001b[A\n",
      "Load checkpoint from: ./savedmodel/wav2lip_checkpoint_step000001100.pth\n",
      "Model loaded\n",
      "100%|█████████████████████████████████████████████| 3/3 [02:03<00:00, 41.24s/it]\n",
      "ffmpeg version 4.2.3 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "  built with gcc 7.5.0 (crosstool-NG 1.24.0.123_1667d2b)\n",
      "  configuration: --prefix=/home/conda/feedstock_root/build_artifacts/ffmpeg_1590573566052/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1590573566052/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "\u001b[0;33mGuessed Channel Layout for Input Stream #0.0 : stereo\n",
      "\u001b[0mInput #0, wav, from '../test.wav':\n",
      "  Metadata:\n",
      "    encoder         : Lavf60.3.100\n",
      "  Duration: 00:00:11.69, bitrate: 1536 kb/s\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, stereo, s16, 1536 kb/s\n",
      "Input #1, avi, from 'temp/result.avi':\n",
      "  Metadata:\n",
      "    encoder         : Lavf59.27.100\n",
      "  Duration: 00:00:11.58, start: 0.000000, bitrate: 10890 kb/s\n",
      "    Stream #1:0: Video: mpeg4 (Simple Profile) (DIVX / 0x58564944), yuv420p, 720x1280 [SAR 1:1 DAR 9:16], 10912 kb/s, 30.14 fps, 30 tbr, 30.14 tbn, 3768 tbc\n",
      "Stream mapping:\n",
      "  Stream #1:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
      "  Stream #0:0 -> #0:1 (pcm_s16le (native) -> aac (native))\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0m\u001b[0;33m-qscale is ignored, -crf is recommended.\n",
      "\u001b[0m\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0musing SAR=1/1\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0mprofile High, level 3.1\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0m264 - core 152 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=12 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'results/result_voice.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p(progressive), 720x1280 [SAR 1:1 DAR 9:16], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "    Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 128 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 aac\n",
      "frame=  349 fps= 58 q=-1.0 Lsize=    5076kB time=00:00:11.69 bitrate=3557.2kbits/s speed=1.94x    \n",
      "video:4878kB audio:186kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.250980%\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0mframe I:2     Avg QP:22.07  size: 72184\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0mframe P:229   Avg QP:24.03  size: 19088\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0mframe B:118   Avg QP:27.38  size:  4056\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0mconsecutive B-frames: 50.4% 10.9%  7.7% 30.9%\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0mmb I  I16..4: 16.9% 76.0%  7.1%\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0mmb P  I16..4:  2.1%  6.1%  0.3%  P16..4: 47.4% 12.2%  9.1%  0.0%  0.0%    skip:22.8%\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0mmb B  I16..4:  0.6%  2.0%  0.0%  B16..8: 46.7%  3.1%  0.4%  direct: 0.6%  skip:46.6%  L0:48.6% L1:38.2% BI:13.2%\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0m8x8 transform intra:72.5% inter:84.2%\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0mcoded y,uvDC,uvAC intra: 44.1% 56.7% 5.1% inter: 20.2% 10.6% 0.1%\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0mi16 v,h,dc,p: 29% 26% 23% 22%\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 26% 22% 37%  3%  2%  3%  2%  2%  2%\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 41% 20% 12%  4%  5%  7%  4%  5%  3%\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0mi8c dc,h,v,p: 49% 22% 25%  4%\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0mWeighted P-Frames: Y:7.9% UV:0.0%\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0mref P L0: 72.3% 12.8% 11.4%  3.5%  0.1%\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0mref B L0: 88.2%  9.4%  2.4%\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0mref B L1: 96.1%  3.9%\n",
      "\u001b[1;36m[libx264 @ 0x55d773b54e40] \u001b[0mkb/s:3434.33\n",
      "\u001b[1;36m[aac @ 0x55d773b55840] \u001b[0mQavg: 1004.547\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python inference.py --checkpoint_path ./savedmodel/wav2lip_checkpoint_step000001100.pth --face ../VID20230620105517.mp4 --audio ../test.wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89403387-1a29-417f-87c2-1b98e48a902e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference.\n",
      "Reading video frames...\n",
      "Number of frames available for inference: 57\n",
      "(80, 185)\n",
      "Length of mel chunks: 55\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|███████████▎                                 | 1/4 [00:02<00:07,  2.35s/it]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.11s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:02<00:00,  1.42it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:03<00:00,  1.17it/s]\u001b[A\n",
      "Load checkpoint from: ./savedmodel/wav2lip_checkpoint_step000001100.pth\n",
      "Model loaded\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:13<00:00, 13.60s/it]\n",
      "ffmpeg version 4.2.3 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "  built with gcc 7.5.0 (crosstool-NG 1.24.0.123_1667d2b)\n",
      "  configuration: --prefix=/home/conda/feedstock_root/build_artifacts/ffmpeg_1590573566052/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1590573566052/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "\u001b[0;33mGuessed Channel Layout for Input Stream #0.0 : mono\n",
      "\u001b[0mInput #0, wav, from '../LSR2/lrs2_preprocessed_288x288-demo/5539702505926936192/00001/audio.wav':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.29.100\n",
      "  Duration: 00:00:02.30, bitrate: 256 kb/s\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
      "Input #1, avi, from 'temp/result.avi':\n",
      "  Metadata:\n",
      "    encoder         : Lavf59.27.100\n",
      "  Duration: 00:00:02.20, start: 0.000000, bitrate: 332 kb/s\n",
      "    Stream #1:0: Video: mpeg4 (Simple Profile) (DIVX / 0x58564944), yuv420p, 160x160 [SAR 1:1 DAR 1:1], 312 kb/s, 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
      "Stream mapping:\n",
      "  Stream #1:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
      "  Stream #0:0 -> #0:1 (pcm_s16le (native) -> aac (native))\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0m\u001b[0;33m-qscale is ignored, -crf is recommended.\n",
      "\u001b[0m\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0musing SAR=1/1\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0mprofile High, level 1.1\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0m264 - core 152 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=5 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'results/result_voice.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p(progressive), 160x160 [SAR 1:1 DAR 1:1], q=-1--1, 25 fps, 12800 tbn, 25 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "    Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 16000 Hz, mono, fltp, 69 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 aac\n",
      "frame=   55 fps=0.0 q=-1.0 Lsize=      58kB time=00:00:02.30 bitrate= 205.6kbits/s speed=25.6x    \n",
      "video:36kB audio:20kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 4.333504%\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0mframe I:1     Avg QP:25.18  size:  3003\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0mframe P:51    Avg QP:25.97  size:   638\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0mframe B:3     Avg QP:30.43  size:   158\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0mconsecutive B-frames: 89.1% 10.9%  0.0%  0.0%\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0mmb I  I16..4:  1.0% 92.0%  7.0%\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0mmb P  I16..4:  0.0%  0.8%  0.0%  P16..4: 46.9% 29.4% 14.9%  0.0%  0.0%    skip: 7.9%\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0mmb B  I16..4:  0.0%  0.0%  0.0%  B16..8: 57.7%  6.0%  0.7%  direct: 1.7%  skip:34.0%  L0:47.7% L1:35.8% BI:16.6%\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0m8x8 transform intra:92.4% inter:69.1%\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0mcoded y,uvDC,uvAC intra: 96.5% 99.3% 73.6% inter: 36.7% 39.0% 2.8%\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0mi16 v,h,dc,p: 50%  0%  0% 50%\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 29% 14% 19%  3%  4% 11%  3% 10%  7%\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 24% 11%  8%  6% 15% 17% 10%  4%  5%\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0mi8c dc,h,v,p: 44% 18% 24% 14%\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0mref P L0: 73.5% 20.8%  4.1%  1.6%\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0mref B L0: 93.5%  6.5%\n",
      "\u001b[1;36m[libx264 @ 0x559e1065e080] \u001b[0mkb/s:130.91\n",
      "\u001b[1;36m[aac @ 0x559e1063c2c0] \u001b[0mQavg: 136.509\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python inference.py --checkpoint_path ./savedmodel/wav2lip_checkpoint_step000001100.pth --face ../LSR2/demo/5539702505926936192/00001.mp4 --audio ../LSR2/lrs2_preprocessed_288x288-demo/5539702505926936192/00001/audio.wav\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6126c1",
   "metadata": {
    "papermill": {
     "duration": 0.039921,
     "end_time": "2021-08-02T15:03:05.798058",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.758137",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. 模型训练\n",
    "模型的训练主要分为两个部分：\n",
    "1. Lip-Sync Expert Discriminator的训练。这里提供官方的预训练模型 [weight](https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EQRvmiZg-HRAjvI6zqN9eTEBP74KefynCwPWVmF57l-AYA?e=ZRPHKP)\n",
    "2. Wav2Lip 模型的训练。\n",
    "\n",
    "### 3.1 预训练Lip-Sync Expert\n",
    "#### 1. 网络的搭建 \n",
    "上面我们已经介绍了SyncNet的基本网络结构，主要有一系列的(Conv+BatchNorm+Relu)组成，这里我们对其进行了一些改进，加入了残差结构。为了方便之后的使用，我们对(Conv+BatchNorm+Relu)以及残差模块进行了封装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab9bf4e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T15:03:05.982454Z",
     "iopub.status.busy": "2021-08-02T15:03:05.981576Z",
     "iopub.status.idle": "2021-08-02T15:03:05.994328Z",
     "shell.execute_reply": "2021-08-02T15:03:05.995013Z",
     "shell.execute_reply.started": "2021-07-30T15:10:55.205557Z"
    },
    "papermill": {
     "duration": 0.15723,
     "end_time": "2021-08-02T15:03:05.995228",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.837998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/wave2lip/wav2lip_homework\n"
     ]
    }
   ],
   "source": [
    "%cd ../input/wave2lip/wav2lip_homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d42226b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T15:03:06.085467Z",
     "iopub.status.busy": "2021-08-02T15:03:06.084752Z",
     "iopub.status.idle": "2021-08-02T15:03:07.548713Z",
     "shell.execute_reply": "2021-08-02T15:03:07.548085Z",
     "shell.execute_reply.started": "2021-07-30T15:10:59.610288Z"
    },
    "papermill": {
     "duration": 1.512392,
     "end_time": "2021-08-02T15:03:07.548865",
     "exception": false,
     "start_time": "2021-08-02T15:03:06.036473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        ########TODO######################\n",
    "        # 按下面的网络结构要求，补全代码\n",
    "        # self.conv_block: Sequential结构，Conv2d+BatchNorm\n",
    "        # self.act: relu激活函数\n",
    "        self.conv_block = nn.Sequential(\n",
    "                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\n",
    "                            nn.BatchNorm2d(cout)\n",
    "                            )\n",
    "        self.act = nn.ReLU()\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block(x)\n",
    "        if self.residual:\n",
    "            out += x\n",
    "        return self.act(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea2698a",
   "metadata": {
    "papermill": {
     "duration": 0.040308,
     "end_time": "2021-08-02T15:03:07.630762",
     "exception": false,
     "start_time": "2021-08-02T15:03:07.590454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "SyncNet的主要包含两个部分：Face_encoder和Audio_encoder。每一个部分都由多个Conv2d模块组成，通过指定卷积核的大小实现对输入的下采样和特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "978ff17b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T15:03:07.732580Z",
     "iopub.status.busy": "2021-08-02T15:03:07.728156Z",
     "iopub.status.idle": "2021-08-02T15:03:07.735885Z",
     "shell.execute_reply": "2021-08-02T15:03:07.735315Z",
     "shell.execute_reply.started": "2021-07-30T15:11:04.53338Z"
    },
    "papermill": {
     "duration": 0.065911,
     "end_time": "2021-08-02T15:03:07.736041",
     "exception": false,
     "start_time": "2021-08-02T15:03:07.670130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class SyncNet_color(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SyncNet_color, self).__init__()\n",
    "        \n",
    "        ################TODO###################\n",
    "        #根据上面提供的网络结构图，补全下面卷积网络的参数\n",
    "\n",
    "        self.face_encoder = nn.Sequential(\n",
    "            Conv2d(15, 32, kernel_size=(7, 7), stride=1, padding=3),\n",
    "\n",
    "            Conv2d(32, 64, kernel_size=5, stride=(1, 2), padding=1),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=0),\n",
    "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n",
    "\n",
    "        self.audio_encoder = nn.Sequential(\n",
    "            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),\n",
    "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n",
    "\n",
    "    def forward(self, audio_sequences, face_sequences): # audio_sequences := (B, dim, T)\n",
    "        \n",
    "        #########################TODO#######################\n",
    "        # 正向传播\n",
    "        face_embedding = self.face_encoder(face_sequences)\n",
    "        audio_embedding = self.audio_encoder(audio_sequences)\n",
    "\n",
    "        audio_embedding = audio_embedding.view(audio_embedding.size(0), -1)\n",
    "        face_embedding = face_embedding.view(face_embedding.size(0), -1)\n",
    "\n",
    "        audio_embedding = F.normalize(audio_embedding, p=2, dim=1)\n",
    "        face_embedding = F.normalize(face_embedding, p=2, dim=1)\n",
    "\n",
    "\n",
    "        return audio_embedding, face_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3012f8a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T15:03:07.822702Z",
     "iopub.status.busy": "2021-08-02T15:03:07.821802Z",
     "iopub.status.idle": "2021-08-02T15:03:10.535026Z",
     "shell.execute_reply": "2021-08-02T15:03:10.535630Z",
     "shell.execute_reply.started": "2021-07-30T15:11:11.303526Z"
    },
    "papermill": {
     "duration": 2.759792,
     "end_time": "2021-08-02T15:03:10.535820",
     "exception": false,
     "start_time": "2021-08-02T15:03:07.776028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os.path import dirname, join, basename, isfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import SyncNet_color as SyncNet\n",
    "import audio\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils import data as data_utils\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import os, random, cv2, argparse\n",
    "from hparams import hparams, get_image_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ca431",
   "metadata": {
    "papermill": {
     "duration": 0.042466,
     "end_time": "2021-08-02T15:03:10.619249",
     "exception": false,
     "start_time": "2021-08-02T15:03:10.576783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2.数据集的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e0eba33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T15:03:10.759060Z",
     "iopub.status.busy": "2021-08-02T15:03:10.758110Z",
     "iopub.status.idle": "2021-08-02T15:03:10.762600Z",
     "shell.execute_reply": "2021-08-02T15:03:10.763266Z",
     "shell.execute_reply.started": "2021-07-30T15:11:14.929415Z"
    },
    "papermill": {
     "duration": 0.103402,
     "end_time": "2021-08-02T15:03:10.763469",
     "exception": false,
     "start_time": "2021-08-02T15:03:10.660067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "global_step = 0 #起始的step\n",
    "global_epoch = 0 #起始的epoch\n",
    "use_cuda = torch.cuda.is_available()#训练的设备 cpu or gpu\n",
    "print('use_cuda: {}'.format(use_cuda))\n",
    "\n",
    "syncnet_T = 5 ## 每次选取200ms的视频片段进行训练，视频的fps为25，所以200ms对应的帧数为：25*0.2=5帧\n",
    "syncnet_mel_step_size = 16 # 200ms对应的声音的mel-spectrogram特征的长度为16.\n",
    "data_root=\"/kaggle/input/wav2lippreprocessed/lrs2_preprocessed\" #数据集的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35646995",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T15:03:10.866626Z",
     "iopub.status.busy": "2021-08-02T15:03:10.865387Z",
     "iopub.status.idle": "2021-08-02T15:03:10.868249Z",
     "shell.execute_reply": "2021-08-02T15:03:10.868927Z",
     "shell.execute_reply.started": "2021-07-30T15:18:29.73854Z"
    },
    "papermill": {
     "duration": 0.065143,
     "end_time": "2021-08-02T15:03:10.869112",
     "exception": false,
     "start_time": "2021-08-02T15:03:10.803969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, split):\n",
    "        self.all_videos = get_image_list(data_root, split)\n",
    "\n",
    "    def get_frame_id(self, frame):\n",
    "        return int(basename(frame).split('.')[0])\n",
    "\n",
    "    def get_window(self, start_frame):\n",
    "        start_id = self.get_frame_id(start_frame)\n",
    "        vidname = dirname(start_frame)\n",
    "\n",
    "        window_fnames = []\n",
    "        for frame_id in range(start_id, start_id + syncnet_T):\n",
    "            frame = join(vidname, '{}.jpg'.format(frame_id))\n",
    "            if not isfile(frame):\n",
    "                return None\n",
    "            window_fnames.append(frame)\n",
    "        return window_fnames\n",
    "\n",
    "    def crop_audio_window(self, spec, start_frame):\n",
    "        # num_frames = (T x hop_size * fps) / sample_rate\n",
    "        start_frame_num = self.get_frame_id(start_frame)\n",
    "        start_idx = int(80. * (start_frame_num / float(hparams.fps)))\n",
    "\n",
    "        end_idx = start_idx + syncnet_mel_step_size\n",
    "\n",
    "        return spec[start_idx : end_idx, :]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        return: x,mel,y\n",
    "        x: 五张嘴唇图片\n",
    "        mel：对应的语音的mel spectrogram\n",
    "        t：同步or不同步\n",
    "        \n",
    "        \"\"\"\n",
    "        while 1:\n",
    "            idx = random.randint(0, len(self.all_videos) - 1)\n",
    "            vidname = self.all_videos[idx]\n",
    "\n",
    "            img_names = list(glob(join(vidname, '*.jpg')))\n",
    "            if len(img_names) <= 3 * syncnet_T:\n",
    "                continue\n",
    "            img_name = random.choice(img_names)\n",
    "            wrong_img_name = random.choice(img_names)\n",
    "            while wrong_img_name == img_name:\n",
    "                wrong_img_name = random.choice(img_names)\n",
    "            \n",
    "            \n",
    "            #随机决定是产生负样本还是正样本\n",
    "            if random.choice([True, False]):\n",
    "                y = torch.ones(1).float()\n",
    "                chosen = img_name\n",
    "            else:\n",
    "                y = torch.zeros(1).float()\n",
    "                chosen = wrong_img_name\n",
    "\n",
    "            window_fnames = self.get_window(chosen)\n",
    "            if window_fnames is None:\n",
    "                continue\n",
    "\n",
    "            window = []\n",
    "            all_read = True\n",
    "            for fname in window_fnames:\n",
    "                img = cv2.imread(fname)\n",
    "                if img is None:\n",
    "                    all_read = False\n",
    "                    break\n",
    "                try:\n",
    "                    img = cv2.resize(img, (hparams.img_size, hparams.img_size))\n",
    "                except Exception as e:\n",
    "                    all_read = False\n",
    "                    break\n",
    "\n",
    "                window.append(img)\n",
    "\n",
    "            if not all_read: continue\n",
    "\n",
    "            try:\n",
    "                wavpath = join(vidname, \"audio.wav\")\n",
    "                wav = audio.load_wav(wavpath, hparams.sample_rate)\n",
    "\n",
    "                orig_mel = audio.melspectrogram(wav).T\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            mel = self.crop_audio_window(orig_mel.copy(), img_name)\n",
    "\n",
    "            if (mel.shape[0] != syncnet_mel_step_size):\n",
    "                continue\n",
    "\n",
    "            # H x W x 3 * T\n",
    "            x = np.concatenate(window, axis=2) / 255.\n",
    "            x = x.transpose(2, 0, 1)\n",
    "            x = x[:, x.shape[1]//2:]\n",
    "\n",
    "            x = torch.FloatTensor(x)\n",
    "            mel = torch.FloatTensor(mel.T).unsqueeze(0)\n",
    "\n",
    "            return x, mel, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ec595dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T15:03:10.956091Z",
     "iopub.status.busy": "2021-08-02T15:03:10.955418Z",
     "iopub.status.idle": "2021-08-02T15:03:11.206554Z",
     "shell.execute_reply": "2021-08-02T15:03:11.208132Z",
     "shell.execute_reply.started": "2021-07-30T15:18:33.470434Z"
    },
    "papermill": {
     "duration": 0.298523,
     "end_time": "2021-08-02T15:03:11.208470",
     "exception": false,
     "start_time": "2021-08-02T15:03:10.909947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 48, 96])\n",
      "torch.Size([1, 80, 16])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "ds=Dataset(\"train\")\n",
    "x,mel,t=ds[0]\n",
    "print(x.shape)\n",
    "print(mel.shape)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe9e76b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T15:03:11.369487Z",
     "iopub.status.busy": "2021-08-02T15:03:11.368246Z",
     "iopub.status.idle": "2021-08-02T15:03:11.547200Z",
     "shell.execute_reply": "2021-08-02T15:03:11.547763Z",
     "shell.execute_reply.started": "2021-07-30T15:18:36.933382Z"
    },
    "papermill": {
     "duration": 0.258701,
     "end_time": "2021-08-02T15:03:11.547945",
     "exception": false,
     "start_time": "2021-08-02T15:03:11.289244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3e8c4b2810>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(mel[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9935eb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T15:03:11.661730Z",
     "iopub.status.busy": "2021-08-02T15:03:11.660550Z",
     "iopub.status.idle": "2021-08-02T15:03:11.806688Z",
     "shell.execute_reply": "2021-08-02T15:03:11.807246Z",
     "shell.execute_reply.started": "2021-07-30T15:18:56.039099Z"
    },
    "papermill": {
     "duration": 0.216749,
     "end_time": "2021-08-02T15:03:11.807488",
     "exception": false,
     "start_time": "2021-08-02T15:03:11.590739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3e8c3b5650>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[:3,:,:].transpose(0,2).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17af9760",
   "metadata": {
    "papermill": {
     "duration": 0.043936,
     "end_time": "2021-08-02T15:03:11.896176",
     "exception": false,
     "start_time": "2021-08-02T15:03:11.852240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 3.训练\n",
    "\n",
    "使用cosine_loss 作为损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "899ae144",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T15:03:11.990593Z",
     "iopub.status.busy": "2021-08-02T15:03:11.989466Z",
     "iopub.status.idle": "2021-08-02T15:03:11.993113Z",
     "shell.execute_reply": "2021-08-02T15:03:11.992583Z",
     "shell.execute_reply.started": "2021-07-30T15:19:00.674203Z"
    },
    "papermill": {
     "duration": 0.053334,
     "end_time": "2021-08-02T15:03:11.993263",
     "exception": false,
     "start_time": "2021-08-02T15:03:11.939929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#损失函数的定义\n",
    "logloss = nn.BCELoss() # 交叉熵损失\n",
    "def cosine_loss(a, v, y):#余弦相似度损失\n",
    "    \"\"\"\n",
    "    a: audio_encoder的输出\n",
    "    v: video face_encoder的输出\n",
    "    y: 是否同步的真实值\n",
    "    \"\"\"\n",
    "    d = nn.functional.cosine_similarity(a, v)\n",
    "    loss = logloss(d.unsqueeze(1), y)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbf7963d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T15:03:12.104793Z",
     "iopub.status.busy": "2021-08-02T15:03:12.103547Z",
     "iopub.status.idle": "2021-08-02T15:03:12.107020Z",
     "shell.execute_reply": "2021-08-02T15:03:12.106428Z",
     "shell.execute_reply.started": "2021-07-30T15:19:09.309393Z"
    },
    "papermill": {
     "duration": 0.06938,
     "end_time": "2021-08-02T15:03:12.107173",
     "exception": false,
     "start_time": "2021-08-02T15:03:12.037793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(device, model, train_data_loader, test_data_loader, optimizer,\n",
    "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n",
    "\n",
    "    global global_step, global_epoch\n",
    "    resumed_step = global_step\n",
    "    \n",
    "    while global_epoch < nepochs:\n",
    "        running_loss = 0.\n",
    "        prog_bar = tqdm(enumerate(train_data_loader))\n",
    "        for step, (x, mel, y) in prog_bar:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #####TODO###########\n",
    "            ####################\n",
    "            #补全模型的训练\n",
    "            x = x.to(device)\n",
    "\n",
    "            mel = mel.to(device)\n",
    "\n",
    "            a, v = model(mel, x)\n",
    "            y = y.to(device)\n",
    "\n",
    "            loss = cosine_loss(a, v, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "            \n",
    "\n",
    "            global_step += 1\n",
    "            cur_session_steps = global_step - resumed_step\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if global_step == 1 or global_step % checkpoint_interval == 0:\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, global_step, checkpoint_dir, global_epoch)\n",
    "\n",
    "            if global_step % hparams.syncnet_eval_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    eval_model(test_data_loader, global_step, device, model, checkpoint_dir)\n",
    "\n",
    "            prog_bar.set_description('Epoch: {} Loss: {}'.format(global_epoch, running_loss / (step + 1)))\n",
    "\n",
    "        global_epoch += 1\n",
    "\n",
    "def eval_model(test_data_loader, global_step, device, model, checkpoint_dir):\n",
    "    #在测试集上进行评估\n",
    "    eval_steps = 1400\n",
    "    print('Evaluating for {} steps'.format(eval_steps))\n",
    "    losses = []\n",
    "    while 1:\n",
    "        for step, (x, mel, y) in enumerate(test_data_loader):\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            # Transform data to CUDA device\n",
    "            x = x.to(device)\n",
    "\n",
    "            mel = mel.to(device)\n",
    "\n",
    "            a, v = model(mel, x)\n",
    "            y = y.to(device)\n",
    "\n",
    "            loss = cosine_loss(a, v, y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if step > eval_steps: break\n",
    "\n",
    "        averaged_loss = sum(losses) / len(losses)\n",
    "        print(averaged_loss)\n",
    "\n",
    "        return\n",
    "\n",
    "latest_checkpoint_path = ''\n",
    "def save_checkpoint(model, optimizer, step, checkpoint_dir, epoch):\n",
    "    #保存训练的结果 checkpoint\n",
    "    global latest_checkpoint_path\n",
    "    \n",
    "    checkpoint_path = join(\n",
    "        checkpoint_dir, \"checkpoint_step{:09d}.pth\".format(global_step))\n",
    "    optimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n",
    "    torch.save({\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer_state,\n",
    "        \"global_step\": step,\n",
    "        \"global_epoch\": epoch,\n",
    "    }, checkpoint_path)\n",
    "    latest_checkpoint_path = checkpoint_path\n",
    "    print(\"Saved checkpoint:\", checkpoint_path)\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "    if use_cuda:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, reset_optimizer=False):\n",
    "    #读取指定checkpoint的保存信息\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    global_step = checkpoint[\"global_step\"]\n",
    "    global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda5f8e",
   "metadata": {
    "papermill": {
     "duration": 0.044378,
     "end_time": "2021-08-02T15:03:12.196392",
     "exception": false,
     "start_time": "2021-08-02T15:03:12.152014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**下面开始训练，最终的Loss参考值为0.20左右，此时模型能达到较好的判别效果**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d3fc627",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T15:03:12.295937Z",
     "iopub.status.busy": "2021-08-02T15:03:12.294854Z",
     "iopub.status.idle": "2021-08-02T17:55:52.010734Z",
     "shell.execute_reply": "2021-08-02T17:55:52.011264Z",
     "shell.execute_reply.started": "2021-07-30T15:19:14.2154Z"
    },
    "papermill": {
     "duration": 10359.77037,
     "end_time": "2021-08-02T17:55:52.011482",
     "exception": false,
     "start_time": "2021-08-02T15:03:12.241112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params 16435072\n",
      "Load checkpoint from: /kaggle/input/wav2lip24epoch/expert_checkpoints/checkpoint_step000060000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 20 Loss: 0.4835729799709603: : 2865it [26:26,  1.81it/s]\n",
      "Epoch: 21 Loss: 0.4813423672791759: : 2865it [24:30,  1.95it/s]\n",
      "Epoch: 22 Loss: 0.4767525762319565: : 2865it [25:10,  1.90it/s]\n",
      "Epoch: 23 Loss: 0.47215773716059506: : 1404it [12:02,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: /kaggle/working/expert_checkpoints/checkpoint_step000070000.pth\n",
      "Evaluating for 1400 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 23 Loss: 0.472245811430438: : 1406it [12:32,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.447623890988967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 23 Loss: 0.4746017324830849: : 2865it [24:56,  1.91it/s]\n",
      "Epoch: 24 Loss: 0.4683841840134865: : 2865it [24:07,  1.98it/s]\n",
      "Epoch: 25 Loss: 0.4698803570882188: : 2865it [23:36,  2.02it/s]\n",
      "Epoch: 26 Loss: 0.4691496957299038: : 2809it [22:58,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: /kaggle/working/expert_checkpoints/checkpoint_step000080000.pth\n",
      "Evaluating for 1400 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 26 Loss: 0.46914908886803974: : 2811it [23:25,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41562949712662134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 26 Loss: 0.46859301660489455: : 2865it [23:38,  2.02it/s]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"/kaggle/working/expert_checkpoints/\" #指定存储 checkpoint的位置\n",
    "checkpoint_path = '/kaggle/input/wav2lip24epoch/expert_checkpoints/checkpoint_step000060000.pth'\n",
    "# 指定加载checkpoint的路径，第一次训练时不需要，后续如果想从某个checkpoint恢复训练，可指定。\n",
    "\n",
    "if not os.path.exists(checkpoint_dir): os.mkdir(checkpoint_dir)\n",
    "\n",
    "# Dataset and Dataloader setup\n",
    "train_dataset = Dataset('train')\n",
    "test_dataset = Dataset('val')\n",
    "\n",
    "############TODO#########\n",
    "#####Train Dataloader and Test Dataloader \n",
    "#### 具体的bacthsize等参数，参考 hparams.py文件\n",
    "train_data_loader = data_utils.DataLoader(\n",
    "    train_dataset, batch_size=hparams.batch_size, shuffle=True,\n",
    "    num_workers=hparams.num_workers)\n",
    "\n",
    "test_data_loader = data_utils.DataLoader(\n",
    "    test_dataset, batch_size=hparams.batch_size,\n",
    "    num_workers=8)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Model\n",
    "#####定义 SynNet模型，并加载到指定的device上\n",
    "model = SyncNet().to(device)\n",
    "print('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "####定义优化器，使用adam，lr参考hparams.py文件\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                       lr=1e-5)\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    load_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=True)\n",
    "\n",
    "train(device, model, train_data_loader, test_data_loader, optimizer,\n",
    "      checkpoint_dir=checkpoint_dir,\n",
    "      checkpoint_interval=hparams.syncnet_checkpoint_interval,\n",
    "      nepochs=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbb81d0",
   "metadata": {
    "papermill": {
     "duration": 13.435605,
     "end_time": "2021-08-02T17:56:19.635821",
     "exception": false,
     "start_time": "2021-08-02T17:56:06.200216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.2 训练Wav2Lip\n",
    "预训练模型 [weight](https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA?e=n9ljGW)\n",
    "#### 1. 模型的定义\n",
    "wav2lip模型的生成器首先对输入进行下采样，然后再经过上采样恢复成原来的大小。为了方便，我们对其中重复利用到的模块进行了封装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfcb2263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T17:56:47.960224Z",
     "iopub.status.busy": "2021-08-02T17:56:47.959101Z",
     "iopub.status.idle": "2021-08-02T17:56:47.962406Z",
     "shell.execute_reply": "2021-08-02T17:56:47.961887Z",
     "shell.execute_reply.started": "2021-07-25T08:00:59.2366Z"
    },
    "papermill": {
     "duration": 13.875945,
     "end_time": "2021-08-02T17:56:47.962555",
     "exception": false,
     "start_time": "2021-08-02T17:56:34.086610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class nonorm_Conv2d(nn.Module): #不需要进行 norm的卷积\n",
    "    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv_block = nn.Sequential(\n",
    "                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\n",
    "                            )\n",
    "        self.act = nn.LeakyReLU(0.01, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block(x)\n",
    "        return self.act(out)\n",
    "\n",
    "class Conv2dTranspose(nn.Module):# 逆卷积，上采样\n",
    "    def __init__(self, cin, cout, kernel_size, stride, padding, output_padding=0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        ############TODO###########\n",
    "        ## 完成self.conv_block: 一个逆卷积和batchnorm组成的 Sequential结构\n",
    "        self.conv_block = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(cin, cout, kernel_size, stride, padding, output_padding),\n",
    "                            nn.BatchNorm2d(cout)\n",
    "                            )\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block(x)\n",
    "        return self.act(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4ba9ce",
   "metadata": {
    "papermill": {
     "duration": 14.540959,
     "end_time": "2021-08-02T17:57:16.146688",
     "exception": false,
     "start_time": "2021-08-02T17:57:01.605729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**生成器**  \n",
    "由两个encoder: face_encoder和 audio_encoder, 一个decoder：face_decoder组成。face encoder 和 audio encoder 分别对输入的人脸和语音特征进行降维，得到（1，1，512）的特征，并将二者进行拼接送入到 face decoder中去进行上采样，最终得到和输入一样大小的人脸图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81cd15af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T17:57:44.098996Z",
     "iopub.status.busy": "2021-08-02T17:57:44.095245Z",
     "iopub.status.idle": "2021-08-02T17:57:44.285370Z",
     "shell.execute_reply": "2021-08-02T17:57:44.284795Z",
     "shell.execute_reply.started": "2021-07-25T08:00:04.850039Z"
    },
    "papermill": {
     "duration": 14.510417,
     "end_time": "2021-08-02T17:57:44.285602",
     "exception": false,
     "start_time": "2021-08-02T17:57:29.775185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#####################TODO############################\n",
    "#根据下面打印的网络模型图，补全网络的参数\n",
    "\n",
    "class Wav2Lip(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Wav2Lip, self).__init__()\n",
    "\n",
    "        self.face_encoder_blocks = nn.ModuleList([\n",
    "            nn.Sequential(Conv2d(6, 16, kernel_size=7, stride=1, padding=3)), # 96,96\n",
    "\n",
    "            nn.Sequential(Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # 48,48\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True)),\n",
    "\n",
    "            nn.Sequential(Conv2d(32, 64, kernel_size=3, stride=2, padding=1),    # 24,24\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True)),\n",
    "\n",
    "            nn.Sequential(Conv2d(64, 128, kernel_size=3, stride=2, padding=1),   # 12,12\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True)),\n",
    "\n",
    "            nn.Sequential(Conv2d(128, 256, kernel_size=3, stride=2, padding=1),       # 6,6\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True)),\n",
    "\n",
    "            nn.Sequential(Conv2d(256, 512, kernel_size=3, stride=2, padding=1),     # 3,3\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),\n",
    "            \n",
    "            nn.Sequential(Conv2d(512, 512, kernel_size=3, stride=1, padding=0),     # 1, 1\n",
    "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0)),])\n",
    "\n",
    "        self.audio_encoder = nn.Sequential(\n",
    "            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),\n",
    "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n",
    "\n",
    "        self.face_decoder_blocks = nn.ModuleList([\n",
    "            nn.Sequential(Conv2d(512, 512, kernel_size=1, stride=1, padding=0),),\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=1, padding=0), # 3,3\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),), # 6, 6\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(768, 384, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),), # 12, 12\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),), # 24, 24\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(320, 128, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),), # 48, 48\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(160, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),),]) # 96,96\n",
    "\n",
    "        self.output_block = nn.Sequential(Conv2d(80, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(32, 3, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Sigmoid()) \n",
    "\n",
    "    def forward(self, audio_sequences, face_sequences):\n",
    "        # audio_sequences = (B, T, 1, 80, 16)\n",
    "        B = audio_sequences.size(0)\n",
    "\n",
    "        input_dim_size = len(face_sequences.size())\n",
    "        if input_dim_size > 4:\n",
    "            audio_sequences = torch.cat([audio_sequences[:, i] for i in range(audio_sequences.size(1))], dim=0)\n",
    "            face_sequences = torch.cat([face_sequences[:, :, i] for i in range(face_sequences.size(2))], dim=0)\n",
    "\n",
    "        audio_embedding = self.audio_encoder(audio_sequences) # B, 512, 1, 1\n",
    "\n",
    "        feats = []\n",
    "        x = face_sequences\n",
    "        for f in self.face_encoder_blocks:\n",
    "            x = f(x)\n",
    "            feats.append(x)\n",
    "\n",
    "        x = audio_embedding\n",
    "        for f in self.face_decoder_blocks:\n",
    "            x = f(x)\n",
    "            try:\n",
    "                x = torch.cat((x, feats[-1]), dim=1)\n",
    "            except Exception as e:\n",
    "                print(x.size())\n",
    "                print(feats[-1].size())\n",
    "                raise e\n",
    "            \n",
    "            feats.pop()\n",
    "\n",
    "        x = self.output_block(x)\n",
    "\n",
    "        if input_dim_size > 4:\n",
    "            x = torch.split(x, B, dim=0) # [(B, C, H, W)]\n",
    "            outputs = torch.stack(x, dim=2) # (B, C, T, H, W)\n",
    "\n",
    "        else:\n",
    "            outputs = x\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a3c21c",
   "metadata": {
    "papermill": {
     "duration": 14.597044,
     "end_time": "2021-08-02T17:58:12.485202",
     "exception": false,
     "start_time": "2021-08-02T17:57:57.888158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**判别器**  \n",
    "判别器也是由一系列的卷积神经网络组成，输入一张人脸图片，利用face encoder对其进行降维到512维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45fdbec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T17:58:39.292009Z",
     "iopub.status.busy": "2021-08-02T17:58:39.290753Z",
     "iopub.status.idle": "2021-08-02T17:58:39.294229Z",
     "shell.execute_reply": "2021-08-02T17:58:39.293660Z",
     "shell.execute_reply.started": "2021-07-25T08:00:08.961755Z"
    },
    "papermill": {
     "duration": 13.426215,
     "end_time": "2021-08-02T17:58:39.294408",
     "exception": false,
     "start_time": "2021-08-02T17:58:25.868193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "###########TODO##################\n",
    "####补全判别器模型\n",
    "class Wav2Lip_disc_qual(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Wav2Lip_disc_qual, self).__init__()\n",
    "\n",
    "        self.face_encoder_blocks = nn.ModuleList([\n",
    "            nn.Sequential(nonorm_Conv2d(3, 32, kernel_size=7, stride=1, padding=3)), # 48,96\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(32, 64, kernel_size=5, stride=(1, 2), padding=2), # 48,48\n",
    "            nonorm_Conv2d(64, 64, kernel_size=5, stride=1, padding=2)),\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(64, 128, kernel_size=5, stride=2, padding=2),    # 24,24\n",
    "            nonorm_Conv2d(128, 128, kernel_size=5, stride=1, padding=2)),\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(128, 256, kernel_size=5, stride=2, padding=2),   # 12,12\n",
    "            nonorm_Conv2d(256, 256, kernel_size=5, stride=1, padding=2)),\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(256, 512, kernel_size=3, stride=2, padding=1),       # 6,6\n",
    "            nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=1)),\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(512, 512, kernel_size=3, stride=2, padding=1),     # 3,3\n",
    "            nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=1),),\n",
    "            \n",
    "            nn.Sequential(nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=0),     # 1, 1\n",
    "            nonorm_Conv2d(512, 512, kernel_size=1, stride=1, padding=0)),])\n",
    "\n",
    "        self.binary_pred = nn.Sequential(nn.Conv2d(512, 1, kernel_size=1, stride=1, padding=0), nn.Sigmoid())\n",
    "        self.label_noise = .0\n",
    "\n",
    "    def get_lower_half(self, face_sequences):\n",
    "        return face_sequences[:, :, face_sequences.size(2)//2:]\n",
    "\n",
    "    def to_2d(self, face_sequences):\n",
    "        B = face_sequences.size(0)\n",
    "        face_sequences = torch.cat([face_sequences[:, :, i] for i in range(face_sequences.size(2))], dim=0)\n",
    "        return face_sequences\n",
    "\n",
    "    def perceptual_forward(self, false_face_sequences):\n",
    "        false_face_sequences = self.to_2d(false_face_sequences)\n",
    "        false_face_sequences = self.get_lower_half(false_face_sequences)\n",
    "\n",
    "        false_feats = false_face_sequences\n",
    "        for f in self.face_encoder_blocks:\n",
    "            false_feats = f(false_feats)\n",
    "\n",
    "        false_pred_loss = F.binary_cross_entropy(self.binary_pred(false_feats).view(len(false_feats), -1), \n",
    "                                        torch.ones((len(false_feats), 1)).cuda())\n",
    "\n",
    "        return false_pred_loss\n",
    "\n",
    "    def forward(self, face_sequences):\n",
    "        face_sequences = self.to_2d(face_sequences)\n",
    "        face_sequences = self.get_lower_half(face_sequences)\n",
    "\n",
    "        x = face_sequences\n",
    "        for f in self.face_encoder_blocks:\n",
    "            x = f(x)\n",
    "\n",
    "        return self.binary_pred(x).view(len(x), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660366b3",
   "metadata": {
    "papermill": {
     "duration": 13.853732,
     "end_time": "2021-08-02T17:59:07.598712",
     "exception": false,
     "start_time": "2021-08-02T17:58:53.744980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2. 数据集的定义  \n",
    "在训练时，会用到4个数据：\n",
    "1. x:输入的图片\n",
    "2. indiv_mels: 每一张图片所对应语音的mel-spectrogram特征\n",
    "3. mel: 所有帧对应的200ms的语音mel-spectrogram，用于SyncNet进行唇音同步损失的计算\n",
    "4. y:真实的与语音对应的，唇音同步的图片。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47f17ba5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T17:59:35.059342Z",
     "iopub.status.busy": "2021-08-02T17:59:35.058328Z",
     "iopub.status.idle": "2021-08-02T17:59:35.062246Z",
     "shell.execute_reply": "2021-08-02T17:59:35.062838Z",
     "shell.execute_reply.started": "2021-07-25T08:00:11.621386Z"
    },
    "papermill": {
     "duration": 13.395158,
     "end_time": "2021-08-02T17:59:35.063001",
     "exception": false,
     "start_time": "2021-08-02T17:59:21.667843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "global_epoch = 0\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('use_cuda: {}'.format(use_cuda))\n",
    "\n",
    "syncnet_T = 5\n",
    "syncnet_mel_step_size = 16\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, split):\n",
    "        self.all_videos = get_image_list(data_root, split)\n",
    "\n",
    "    def get_frame_id(self, frame):\n",
    "        return int(basename(frame).split('.')[0])\n",
    "\n",
    "    def get_window(self, start_frame):\n",
    "        start_id = self.get_frame_id(start_frame)\n",
    "        vidname = dirname(start_frame)\n",
    "\n",
    "        window_fnames = []\n",
    "        for frame_id in range(start_id, start_id + syncnet_T):\n",
    "            frame = join(vidname, '{}.jpg'.format(frame_id))\n",
    "            if not isfile(frame):\n",
    "                return None\n",
    "            window_fnames.append(frame)\n",
    "        return window_fnames\n",
    "\n",
    "    def read_window(self, window_fnames):\n",
    "        if window_fnames is None: return None\n",
    "        window = []\n",
    "        for fname in window_fnames:\n",
    "            img = cv2.imread(fname)\n",
    "            if img is None:\n",
    "                return None\n",
    "            try:\n",
    "                img = cv2.resize(img, (hparams.img_size, hparams.img_size))\n",
    "            except Exception as e:\n",
    "                return None\n",
    "\n",
    "            window.append(img)\n",
    "\n",
    "        return window\n",
    "\n",
    "    def crop_audio_window(self, spec, start_frame):\n",
    "        if type(start_frame) == int:\n",
    "            start_frame_num = start_frame\n",
    "        else:\n",
    "            start_frame_num = self.get_frame_id(start_frame) # 0-indexing ---> 1-indexing\n",
    "        start_idx = int(80. * (start_frame_num / float(hparams.fps)))\n",
    "        \n",
    "        end_idx = start_idx + syncnet_mel_step_size\n",
    "\n",
    "        return spec[start_idx : end_idx, :]\n",
    "\n",
    "    def get_segmented_mels(self, spec, start_frame):\n",
    "        mels = []\n",
    "        assert syncnet_T == 5\n",
    "        start_frame_num = self.get_frame_id(start_frame) + 1 # 0-indexing ---> 1-indexing\n",
    "        if start_frame_num - 2 < 0: return None\n",
    "        for i in range(start_frame_num, start_frame_num + syncnet_T):\n",
    "            m = self.crop_audio_window(spec, i - 2)\n",
    "            if m.shape[0] != syncnet_mel_step_size:\n",
    "                return None\n",
    "            mels.append(m.T)\n",
    "\n",
    "        mels = np.asarray(mels)\n",
    "\n",
    "        return mels\n",
    "\n",
    "    def prepare_window(self, window):\n",
    "        # 3 x T x H x W\n",
    "        x = np.asarray(window) / 255.\n",
    "        x = np.transpose(x, (3, 0, 1, 2))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while 1:\n",
    "            idx = random.randint(0, len(self.all_videos) - 1) #随机选择一个视频id\n",
    "            vidname = self.all_videos[idx]\n",
    "            img_names = list(glob(join(vidname, '*.jpg')))\n",
    "            if len(img_names) <= 3 * syncnet_T:\n",
    "                continue\n",
    "            \n",
    "            img_name = random.choice(img_names)\n",
    "            wrong_img_name = random.choice(img_names)#随机选择帧\n",
    "            while wrong_img_name == img_name:\n",
    "                wrong_img_name = random.choice(img_names)\n",
    "\n",
    "            window_fnames = self.get_window(img_name)\n",
    "            wrong_window_fnames = self.get_window(wrong_img_name)\n",
    "            if window_fnames is None or wrong_window_fnames is None:\n",
    "                continue\n",
    "\n",
    "            window = self.read_window(window_fnames)\n",
    "            if window is None:\n",
    "                continue\n",
    "\n",
    "            wrong_window = self.read_window(wrong_window_fnames)\n",
    "            if wrong_window is None:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                #读取音频\n",
    "                wavpath = join(vidname, \"audio.wav\")\n",
    "                wav = audio.load_wav(wavpath, hparams.sample_rate)\n",
    "                #提取完整mel-spectrogram\n",
    "                orig_mel = audio.melspectrogram(wav).T\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            # 分割 mel-spectrogram\n",
    "            mel = self.crop_audio_window(orig_mel.copy(), img_name)\n",
    "            \n",
    "            if (mel.shape[0] != syncnet_mel_step_size):\n",
    "                continue\n",
    "\n",
    "            indiv_mels = self.get_segmented_mels(orig_mel.copy(), img_name)\n",
    "            if indiv_mels is None: continue\n",
    "\n",
    "            window = self.prepare_window(window)\n",
    "            y = window.copy()\n",
    "            window[:, :, window.shape[2]//2:] = 0.\n",
    "\n",
    "            wrong_window = self.prepare_window(wrong_window)\n",
    "            x = np.concatenate([window, wrong_window], axis=0)\n",
    "\n",
    "            x = torch.FloatTensor(x)\n",
    "            mel = torch.FloatTensor(mel.T).unsqueeze(0)\n",
    "            indiv_mels = torch.FloatTensor(indiv_mels).unsqueeze(1)\n",
    "            y = torch.FloatTensor(y)\n",
    "            return x, indiv_mels, mel, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e86c7554",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T18:00:03.159945Z",
     "iopub.status.busy": "2021-08-02T18:00:03.159146Z",
     "iopub.status.idle": "2021-08-02T18:00:03.372457Z",
     "shell.execute_reply": "2021-08-02T18:00:03.373748Z",
     "shell.execute_reply.started": "2021-07-25T08:00:15.736819Z"
    },
    "papermill": {
     "duration": 14.328526,
     "end_time": "2021-08-02T18:00:03.374059",
     "exception": false,
     "start_time": "2021-08-02T17:59:49.045533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 5, 96, 96])\n",
      "torch.Size([5, 1, 80, 16])\n",
      "torch.Size([1, 80, 16])\n",
      "torch.Size([3, 5, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "ds=Dataset(\"train\")\n",
    "x, indiv_mels, mel, y=ds[0]\n",
    "print(x.shape)\n",
    "print(indiv_mels.shape)\n",
    "print(mel.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7633b6",
   "metadata": {
    "papermill": {
     "duration": 13.845496,
     "end_time": "2021-08-02T18:00:30.922672",
     "exception": false,
     "start_time": "2021-08-02T18:00:17.077176",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a093d2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T18:00:59.167079Z",
     "iopub.status.busy": "2021-08-02T18:00:59.146198Z",
     "iopub.status.idle": "2021-08-02T18:00:59.375617Z",
     "shell.execute_reply": "2021-08-02T18:00:59.374722Z",
     "shell.execute_reply.started": "2021-07-25T08:00:17.791703Z"
    },
    "papermill": {
     "duration": 13.857818,
     "end_time": "2021-08-02T18:00:59.375763",
     "exception": false,
     "start_time": "2021-08-02T18:00:45.517945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#bce 交叉墒loss\n",
    "logloss = nn.BCELoss()\n",
    "def cosine_loss(a, v, y):\n",
    "    d = nn.functional.cosine_similarity(a, v)\n",
    "    loss = logloss(d.unsqueeze(1), y)\n",
    "\n",
    "    return loss\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "syncnet = SyncNet().to(device) # 定义syncnet 模型\n",
    "for p in syncnet.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "    \n",
    "#####L1 loss    \n",
    "recon_loss = nn.L1Loss()\n",
    "def get_sync_loss(mel, g):\n",
    "    g = g[:, :, :, g.size(3)//2:]\n",
    "    g = torch.cat([g[:, :, i] for i in range(syncnet_T)], dim=1)\n",
    "    # B, 3 * T, H//2, W\n",
    "    a, v = syncnet(mel, g)\n",
    "    y = torch.ones(g.size(0), 1).float().to(device)\n",
    "    return cosine_loss(a, v, y)\n",
    "\n",
    "def train(device, model, disc, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n",
    "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n",
    "    global global_step, global_epoch\n",
    "    resumed_step = global_step\n",
    "\n",
    "    while global_epoch < nepochs:\n",
    "        print('Starting Epoch: {}'.format(global_epoch))\n",
    "        running_sync_loss, running_l1_loss, disc_loss, running_perceptual_loss = 0., 0., 0., 0.\n",
    "        running_disc_real_loss, running_disc_fake_loss = 0., 0.\n",
    "        prog_bar = tqdm(enumerate(train_data_loader))\n",
    "        for step, (x, indiv_mels, mel, gt) in prog_bar:\n",
    "            disc.train()\n",
    "            model.train()\n",
    "\n",
    "            x = x.to(device)\n",
    "            mel = mel.to(device)\n",
    "            indiv_mels = indiv_mels.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            ### Train generator now. Remove ALL grads. \n",
    "            #训练生成器\n",
    "            optimizer.zero_grad()\n",
    "            disc_optimizer.zero_grad()\n",
    "\n",
    "            g = model(indiv_mels, x)#得到生成的结果\n",
    "\n",
    "            if hparams.syncnet_wt > 0.:\n",
    "                sync_loss = get_sync_loss(mel, g)# 从预训练的expert 模型中获得唇音同步的损失\n",
    "            else:\n",
    "                sync_loss = 0.\n",
    "\n",
    "            if hparams.disc_wt > 0.:\n",
    "                perceptual_loss = disc.perceptual_forward(g)#判别器的感知损失\n",
    "            else:\n",
    "                perceptual_loss = 0.\n",
    "\n",
    "            l1loss = recon_loss(g, gt)#l1 loss，重建损失\n",
    "            \n",
    "            #最终的损失函数\n",
    "            loss = hparams.syncnet_wt * sync_loss + hparams.disc_wt * perceptual_loss + \\\n",
    "                                    (1. - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ### Remove all gradients before Training disc\n",
    "            # 训练判别器\n",
    "            disc_optimizer.zero_grad()\n",
    "\n",
    "            pred = disc(gt)\n",
    "            disc_real_loss = F.binary_cross_entropy(pred, torch.ones((len(pred), 1)).to(device))\n",
    "            disc_real_loss.backward()\n",
    "\n",
    "            pred = disc(g.detach())\n",
    "            disc_fake_loss = F.binary_cross_entropy(pred, torch.zeros((len(pred), 1)).to(device))\n",
    "            disc_fake_loss.backward()\n",
    "\n",
    "            disc_optimizer.step()\n",
    "\n",
    "            running_disc_real_loss += disc_real_loss.item()\n",
    "            running_disc_fake_loss += disc_fake_loss.item()\n",
    "\n",
    "            # Logs\n",
    "            global_step += 1\n",
    "            cur_session_steps = global_step - resumed_step\n",
    "\n",
    "            running_l1_loss += l1loss.item()\n",
    "            if hparams.syncnet_wt > 0.:\n",
    "                running_sync_loss += sync_loss.item()\n",
    "            else:\n",
    "                running_sync_loss += 0.\n",
    "\n",
    "            if hparams.disc_wt > 0.:\n",
    "                running_perceptual_loss += perceptual_loss.item()\n",
    "            else:\n",
    "                running_perceptual_loss += 0.\n",
    "\n",
    "            if global_step == 1 or global_step % checkpoint_interval == 0:\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, global_step, checkpoint_dir, global_epoch)\n",
    "                save_checkpoint(disc, disc_optimizer, global_step, checkpoint_dir, global_epoch, prefix='disc_')\n",
    "\n",
    "\n",
    "            if global_step % hparams.eval_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    average_sync_loss = eval_model(test_data_loader, global_step, device, model, disc)\n",
    "\n",
    "                    if average_sync_loss < .75:\n",
    "                        hparams.set_hparam('syncnet_wt', 0.03)\n",
    "\n",
    "            prog_bar.set_description('L1: {}, Sync: {}, Percep: {} | Fake: {}, Real: {}'.format(running_l1_loss / (step + 1),\n",
    "                                                                                        running_sync_loss / (step + 1),\n",
    "                                                                                        running_perceptual_loss / (step + 1),\n",
    "                                                                                        running_disc_fake_loss / (step + 1),\n",
    "                                                                                        running_disc_real_loss / (step + 1)))\n",
    "\n",
    "        global_epoch += 1\n",
    "\n",
    "def eval_model(test_data_loader, global_step, device, model, disc):\n",
    "    eval_steps = 300\n",
    "    print('Evaluating for {} steps'.format(eval_steps))\n",
    "    running_sync_loss, running_l1_loss, running_disc_real_loss, running_disc_fake_loss, running_perceptual_loss = [], [], [], [], []\n",
    "    while 1:\n",
    "        for step, (x, indiv_mels, mel, gt) in enumerate((test_data_loader)):\n",
    "            model.eval()\n",
    "            disc.eval()\n",
    "\n",
    "            x = x.to(device)\n",
    "            mel = mel.to(device)\n",
    "            indiv_mels = indiv_mels.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            pred = disc(gt)\n",
    "            disc_real_loss = F.binary_cross_entropy(pred, torch.ones((len(pred), 1)).to(device))\n",
    "\n",
    "            g = model(indiv_mels, x)\n",
    "            pred = disc(g)\n",
    "            disc_fake_loss = F.binary_cross_entropy(pred, torch.zeros((len(pred), 1)).to(device))\n",
    "\n",
    "            running_disc_real_loss.append(disc_real_loss.item())\n",
    "            running_disc_fake_loss.append(disc_fake_loss.item())\n",
    "\n",
    "            sync_loss = get_sync_loss(mel, g)\n",
    "            \n",
    "            if hparams.disc_wt > 0.:\n",
    "                perceptual_loss = disc.perceptual_forward(g)\n",
    "            else:\n",
    "                perceptual_loss = 0.\n",
    "\n",
    "            l1loss = recon_loss(g, gt)\n",
    "\n",
    "            loss = hparams.syncnet_wt * sync_loss + hparams.disc_wt * perceptual_loss + \\\n",
    "                                    (1. - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n",
    "\n",
    "            running_l1_loss.append(l1loss.item())\n",
    "            running_sync_loss.append(sync_loss.item())\n",
    "            \n",
    "            if hparams.disc_wt > 0.:\n",
    "                running_perceptual_loss.append(perceptual_loss.item())\n",
    "            else:\n",
    "                running_perceptual_loss.append(0.)\n",
    "\n",
    "            if step > eval_steps: break\n",
    "\n",
    "        print('L1: {}, Sync: {}, Percep: {} | Fake: {}, Real: {}'.format(sum(running_l1_loss) / len(running_l1_loss),\n",
    "                                                            sum(running_sync_loss) / len(running_sync_loss),\n",
    "                                                            sum(running_perceptual_loss) / len(running_perceptual_loss),\n",
    "                                                            sum(running_disc_fake_loss) / len(running_disc_fake_loss),\n",
    "                                                             sum(running_disc_real_loss) / len(running_disc_real_loss)))\n",
    "        return sum(running_sync_loss) / len(running_sync_loss)\n",
    "\n",
    "latest_wav2lip_checkpoint = ''\n",
    "def save_checkpoint(model, optimizer, step, checkpoint_dir, epoch, prefix=''):\n",
    "    global latest_wav2lip_checkpoint\n",
    "    checkpoint_path = join(\n",
    "        checkpoint_dir, \"{}checkpoint_step{:09d}.pth\".format(prefix, global_step))\n",
    "    if 'disc' not in checkpoint_path:\n",
    "        latest_wav2lip_checkpoint = checkpoint_path\n",
    "    optimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n",
    "    torch.save({\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer_state,\n",
    "        \"global_step\": step,\n",
    "        \"global_epoch\": epoch,\n",
    "    }, checkpoint_path)\n",
    "    print(\"Saved checkpoint:\", checkpoint_path)\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "    if use_cuda:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, reset_optimizer=False, overwrite_global_states=True):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    s = checkpoint[\"state_dict\"]\n",
    "    new_s = {}\n",
    "    for k, v in s.items():\n",
    "        new_s[k.replace('module.', '')] = v\n",
    "    model.load_state_dict(new_s)\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    if overwrite_global_states:\n",
    "        global_step = checkpoint[\"global_step\"]\n",
    "        global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f07edeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T18:01:27.182753Z",
     "iopub.status.busy": "2021-08-02T18:01:27.181977Z",
     "iopub.status.idle": "2021-08-02T23:19:43.358471Z",
     "shell.execute_reply": "2021-08-02T23:19:43.357908Z",
     "shell.execute_reply.started": "2021-07-25T08:01:08.453052Z"
    },
    "papermill": {
     "duration": 19110.49203,
     "end_time": "2021-08-02T23:19:43.358658",
     "exception": false,
     "start_time": "2021-08-02T18:01:12.866628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params 36298035\n",
      "total DISC trainable params 14113793\n",
      "Load checkpoint from: /kaggle/working/expert_checkpoints/checkpoint_step000080000.pth\n",
      "Starting Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/checkpoint_step000000001.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.182265505194664, Sync: 0.0, Percep: 0.6852962374687195 | Fake: 0.7010602951049805, Real: 0.6852961778640747: : 1it [00:17, 17.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/disc_checkpoint_step000000001.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.046039844079375475, Sync: 0.0, Percep: 0.7006118735353776 | Fake: 0.6912129632985613, Real: 0.694671366006188: : 2865it [1:02:50,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "L1: 0.040241426927273845, Sync: 0.0, Percep: 0.6867115430867494 | Fake: 0.703810100679967, Real: 0.6803412684308949: : 134it [03:08,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/checkpoint_step000003000.pth\n",
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/disc_checkpoint_step000003000.pth\n",
      "Evaluating for 300 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.04018904253564499, Sync: 0.0, Percep: 0.6864057969163966 | Fake: 0.7041104413844921, Real: 0.6801264032169625: : 135it [04:04, 17.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1: 0.03741014929597869, Sync: 1.4850089260760475, Percep: 0.6467777455554289 | Fake: 0.7423368034993901, Real: 0.651926989941036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.03900269641562074, Sync: 0.0, Percep: 0.6961903256570585 | Fake: 0.6939434029460994, Real: 0.6915473134879406: : 2865it [1:03:52,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "L1: 0.03545909536289238, Sync: 0.0, Percep: 0.6940031975618526 | Fake: 0.6933492843103232, Real: 0.6925399040865632: : 269it [06:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/checkpoint_step000006000.pth\n",
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/disc_checkpoint_step000006000.pth\n",
      "Evaluating for 300 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.03545024025909327, Sync: 0.0, Percep: 0.6940377164770055 | Fake: 0.6933112680912018, Real: 0.6925840098548819: : 270it [06:52, 16.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1: 0.031579675563775444, Sync: 0.9123614119256244, Percep: 0.7057484519832274 | Fake: 0.6807170221034218, Real: 0.7051262636395061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.03530552504717471, Sync: 0.0, Percep: 0.6969361525971643 | Fake: 0.6927072046991002, Real: 0.6919782925754734: : 2865it [1:03:34,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "L1: 0.03414407477594248, Sync: 0.0, Percep: 0.6948496932440466 | Fake: 0.6945914035976524, Real: 0.6901939750307857: : 404it [09:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/checkpoint_step000009000.pth\n",
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/disc_checkpoint_step000009000.pth\n",
      "Evaluating for 300 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.03419261850692608, Sync: 0.0, Percep: 0.6947742740313212 | Fake: 0.694661506605737, Real: 0.6901227241680946: : 405it [09:53, 17.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1: 0.02947066765388145, Sync: 0.9377104578649297, Percep: 0.6579884185510523 | Fake: 0.7296747687984916, Real: 0.6585180066964206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.03367848934225387, Sync: 0.0, Percep: 0.6942483557768517 | Fake: 0.6947852298643369, Real: 0.6895289202427157: : 2865it [1:03:11,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "L1: 0.03179426823011798, Sync: 0.0, Percep: 0.7001391245615505 | Fake: 0.6897339982314101, Real: 0.6953619473608615: : 539it [12:07,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/checkpoint_step000012000.pth\n",
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/disc_checkpoint_step000012000.pth\n",
      "Evaluating for 300 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.03178744904558967, Sync: 0.0, Percep: 0.7001926060076114 | Fake: 0.6896766474953404, Real: 0.6954020820834018: : 540it [12:58, 16.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1: 0.0268396848867483, Sync: 0.6645076638635468, Percep: 0.7485925362390631 | Fake: 0.6411540850120432, Real: 0.7362593973384184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.0327194768181282, Sync: 0.16193598365284387, Percep: 0.6984948469290142 | Fake: 0.6914800586292673, Real: 0.6937821723091665: : 2865it [1:04:43,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"/kaggle/working/wav2lip_checkpoints\"  #checkpoint 存储的位置\n",
    "\n",
    "# Dataset and Dataloader setup\n",
    "train_dataset = Dataset('train')\n",
    "test_dataset = Dataset('val')\n",
    "\n",
    "train_data_loader = data_utils.DataLoader(\n",
    "    train_dataset, batch_size=hparams.batch_size, shuffle=True,\n",
    "    num_workers=hparams.num_workers)\n",
    "\n",
    "test_data_loader = data_utils.DataLoader(\n",
    "    test_dataset, batch_size=hparams.batch_size,\n",
    "    num_workers=4)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    " # Model\n",
    "model = Wav2Lip().to(device)####### 生成器模型\n",
    "disc = Wav2Lip_disc_qual().to(device)####### 判别器模型\n",
    "\n",
    "print('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "print('total DISC trainable params {}'.format(sum(p.numel() for p in disc.parameters() if p.requires_grad)))\n",
    "\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                       lr=hparams.initial_learning_rate,\n",
    "                       betas=(0.5, 0.999))#####adam优化器，betas=[0.5,0.999]\n",
    "disc_optimizer = optim.Adam([p for p in disc.parameters() if p.requires_grad],\n",
    "                            lr=hparams.disc_initial_learning_rate,\n",
    "                            betas=(0.5, 0.999))#####adam优化器，betas=[0.5,0.999]\n",
    "\n",
    "#继续训练的生成器的checkpoint位置\n",
    "# checkpoint_path=\"\"\n",
    "# load_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=False)\n",
    "#继续训练的判别器的checkpoint位置\n",
    "# disc_checkpoint_path=\"\"\n",
    "# load_checkpoint(disc_checkpoint_path, disc, disc_optimizer, \n",
    "#                             reset_optimizer=False, overwrite_global_states=False)\n",
    "\n",
    "# syncnet的checkpoint位置，我们将使用此模型计算生成的帧和语音的唇音同步损失\n",
    "syncnet_checkpoint_path = latest_checkpoint_path\n",
    "# syncnet_checkpoint_path=\"/kaggle/working/expert_checkpoints/checkpoint_step000000001.pth\"\n",
    "load_checkpoint(syncnet_checkpoint_path, syncnet, None, reset_optimizer=True,\n",
    "                            overwrite_global_states=False)\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "\n",
    "# Train!\n",
    "train(device, model, disc, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n",
    "          checkpoint_dir=checkpoint_dir,\n",
    "          checkpoint_interval=hparams.checkpoint_interval,\n",
    "          nepochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e6ebce",
   "metadata": {
    "papermill": {
     "duration": 23.803383,
     "end_time": "2021-08-02T23:20:31.249791",
     "exception": false,
     "start_time": "2021-08-02T23:20:07.446408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 4. 命令行训练\n",
    "上面是按步骤训练的过程，在`hq_wav2lip_train.py`文件中已经把上述的过程进行了封装，你可以通过以下的命令直接进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d7471b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T23:21:18.612627Z",
     "iopub.status.busy": "2021-08-02T23:21:18.611852Z",
     "iopub.status.idle": "2021-08-02T23:21:18.615898Z",
     "shell.execute_reply": "2021-08-02T23:21:18.615322Z"
    },
    "papermill": {
     "duration": 23.637103,
     "end_time": "2021-08-02T23:21:18.616061",
     "exception": false,
     "start_time": "2021-08-02T23:20:54.978958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python wav2lip_train.py --data_root lrs2_preprocessed/ --checkpoint_dir <folder_to_save_checkpoints> --syncnet_checkpoint_path <path_to_expert_disc_checkpoint>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf58ad3e",
   "metadata": {
    "papermill": {
     "duration": 23.316935,
     "end_time": "2021-08-02T23:22:05.716586",
     "exception": false,
     "start_time": "2021-08-02T23:21:42.399651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4. 模型的推理\n",
    "当模型训练完毕后，我们只使用生成器的网络模型部分作为我们的推理模型。模型的输入由一段包含人脸的参照视频和一段语音组成。  \n",
    "在这里我们可以直接使用官方提供给我们的预训练模型[weight](https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA?e=n9ljGW)下载该模型并放入到指定文件夹下，供之后的推理使用。  \n",
    "模型的推理过程主要分为以下几个步骤：\n",
    "1. 输入数据的预处理，包含人脸抠图，视频分帧，提取mel-spectrogram特征等操作。\n",
    "2. 利用网络模型生成唇音同步的视频帧。\n",
    "3. 将生成的视频帧准换成视频，并和输入的语音结合，形成最终的输出视频。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b055487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T23:22:53.596568Z",
     "iopub.status.busy": "2021-08-02T23:22:53.595901Z",
     "iopub.status.idle": "2021-08-02T23:22:53.676894Z",
     "shell.execute_reply": "2021-08-02T23:22:53.676292Z",
     "shell.execute_reply.started": "2021-07-24T13:57:11.710238Z"
    },
    "papermill": {
     "duration": 23.737003,
     "end_time": "2021-08-02T23:22:53.677033",
     "exception": false,
     "start_time": "2021-08-02T23:22:29.940030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import listdir, path\n",
    "import numpy as np\n",
    "import scipy, cv2, os, sys, argparse, audio\n",
    "import json, subprocess, random, string\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import torch, face_detection\n",
    "from models import Wav2Lip\n",
    "import platform\n",
    "import audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "666c6ad3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T23:23:40.705556Z",
     "iopub.status.busy": "2021-08-02T23:23:40.704379Z",
     "iopub.status.idle": "2021-08-02T23:23:40.708302Z",
     "shell.execute_reply": "2021-08-02T23:23:40.707752Z",
     "shell.execute_reply.started": "2021-07-24T13:59:02.638662Z"
    },
    "papermill": {
     "duration": 24.033897,
     "end_time": "2021-08-02T23:23:40.708477",
     "exception": false,
     "start_time": "2021-08-02T23:23:16.674580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_path=\"/kaggle/working/wav2lip_checkpoints/checkpoint_step000000001.pth\"#生成器的checkpoint位置\n",
    "checkpoint_path = latest_wav2lip_checkpoint\n",
    "face=\"input_video.mp4\" #参照视频的文件位置, *.mp4\n",
    "speech=\"input_audio.wav\"#输入语音的位置，*.wav\n",
    "resize_factor=1 #对输入的视频进行下采样的倍率\n",
    "crop=[0,-1,0,-1] #是否对视频帧进行裁剪,处理视频中有多张人脸时有用\n",
    "fps=25#视频的帧率\n",
    "static=False #是否只使用固定的一帧作为视频的生成参照"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17f5e06f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T23:24:27.735293Z",
     "iopub.status.busy": "2021-08-02T23:24:27.734707Z",
     "iopub.status.idle": "2021-08-02T23:24:28.342689Z",
     "shell.execute_reply": "2021-08-02T23:24:28.342124Z",
     "shell.execute_reply.started": "2021-07-24T13:59:14.496913Z"
    },
    "papermill": {
     "duration": 24.049737,
     "end_time": "2021-08-02T23:24:28.342843",
     "exception": false,
     "start_time": "2021-08-02T23:24:04.293106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading video frames...\n",
      "Number of frames available for inference: 210\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(face):\n",
    "    raise ValueError('--face argument must be a valid path to video/image file')\n",
    "\n",
    "\n",
    "else:# 若输入的是视频格式\n",
    "    video_stream = cv2.VideoCapture(face)# 读取视频\n",
    "    fps = video_stream.get(cv2.CAP_PROP_FPS)# 读取 fps\n",
    "\n",
    "    print('Reading video frames...')\n",
    "\n",
    "    full_frames = []\n",
    "    #提取所有的帧\n",
    "    while 1:\n",
    "        still_reading, frame = video_stream.read()\n",
    "        if not still_reading:\n",
    "            video_stream.release()\n",
    "            break\n",
    "        if resize_factor > 1: # 进行下采样，降低分辨率\n",
    "            frame = cv2.resize(frame, (frame.shape[1]//resize_factor, frame.shape[0]//resize_factor))\n",
    "\n",
    "        \n",
    "\n",
    "        y1, y2, x1, x2 =crop  # 裁剪\n",
    "        if x2 == -1: x2 = frame.shape[1]\n",
    "        if y2 == -1: y2 = frame.shape[0]\n",
    "\n",
    "        frame = frame[y1:y2, x1:x2]\n",
    "\n",
    "        full_frames.append(frame)\n",
    "\n",
    "print (\"Number of frames available for inference: \"+str(len(full_frames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d9a7b4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T23:25:16.117664Z",
     "iopub.status.busy": "2021-08-02T23:25:16.116930Z",
     "iopub.status.idle": "2021-08-02T23:25:18.311151Z",
     "shell.execute_reply": "2021-08-02T23:25:18.310142Z",
     "shell.execute_reply.started": "2021-07-24T13:59:29.319841Z"
    },
    "papermill": {
     "duration": 26.461325,
     "end_time": "2021-08-02T23:25:18.311428",
     "exception": false,
     "start_time": "2021-08-02T23:24:51.850103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 353)\n"
     ]
    }
   ],
   "source": [
    "#检查输入的音频是否为 .wav格式的，若不是则进行转换\n",
    "if not speech.endswith('.wav'):\n",
    "    print('Extracting raw audio...')\n",
    "    command = 'ffmpeg -y -i {} -strict -2 {}'.format(speech, 'temp/temp.wav')\n",
    "\n",
    "    subprocess.call(command, shell=True)\n",
    "    speech = 'temp/temp.wav'\n",
    "\n",
    "wav = audio.load_wav(speech, 16000)#保证采样率为16000\n",
    "mel = audio.melspectrogram(wav)\n",
    "print(mel.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77c8e9cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T23:26:05.517273Z",
     "iopub.status.busy": "2021-08-02T23:26:05.514879Z",
     "iopub.status.idle": "2021-08-02T23:26:05.520158Z",
     "shell.execute_reply": "2021-08-02T23:26:05.519493Z",
     "shell.execute_reply.started": "2021-07-24T13:59:40.598345Z"
    },
    "papermill": {
     "duration": 23.411431,
     "end_time": "2021-08-02T23:26:05.520312",
     "exception": false,
     "start_time": "2021-08-02T23:25:42.108881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of mel chunks: 128\n"
     ]
    }
   ],
   "source": [
    "wav2lip_batch_size=128 #推理时输入到网络的batchsize\n",
    "mel_step_size=16\n",
    "\n",
    "#提取语音的mel谱\n",
    "mel_chunks = []\n",
    "mel_idx_multiplier = 80./fps \n",
    "i = 0\n",
    "while 1:\n",
    "    start_idx = int(i * mel_idx_multiplier)\n",
    "    if start_idx + mel_step_size > len(mel[0]):\n",
    "        mel_chunks.append(mel[:, len(mel[0]) - mel_step_size:])\n",
    "        break\n",
    "    mel_chunks.append(mel[:, start_idx : start_idx + mel_step_size])\n",
    "    i += 1\n",
    "\n",
    "print(\"Length of mel chunks: {}\".format(len(mel_chunks)))\n",
    "\n",
    "full_frames = full_frames[:len(mel_chunks)]\n",
    "\n",
    "batch_size = wav2lip_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0830499",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T23:26:53.275172Z",
     "iopub.status.busy": "2021-08-02T23:26:53.273947Z",
     "iopub.status.idle": "2021-08-02T23:26:53.344058Z",
     "shell.execute_reply": "2021-08-02T23:26:53.345519Z",
     "shell.execute_reply.started": "2021-07-24T13:59:59.813928Z"
    },
    "papermill": {
     "duration": 24.221438,
     "end_time": "2021-08-02T23:26:53.345771",
     "exception": false,
     "start_time": "2021-08-02T23:26:29.124333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference.\n"
     ]
    }
   ],
   "source": [
    "img_size = 96 #默认的输入图片大小\n",
    "pads=[0,20,0,0] # 填充的长度，保证下巴也在抠图的范围之内\n",
    "nosmooth=False\n",
    "face_det_batch_size=16\n",
    "\n",
    "def get_smoothened_boxes(boxes, T):\n",
    "    for i in range(len(boxes)):\n",
    "        if i + T > len(boxes):\n",
    "            window = boxes[len(boxes) - T:]\n",
    "        else:\n",
    "            window = boxes[i : i + T]\n",
    "        boxes[i] = np.mean(window, axis=0)\n",
    "    return boxes\n",
    "\n",
    "#人脸检测函数\n",
    "def face_detect(images):\n",
    "    detector = face_detection.FaceAlignment(face_detection.LandmarksType._2D, \n",
    "                                            flip_input=False, device=device)\n",
    "\n",
    "    batch_size = face_det_batch_size\n",
    "\n",
    "    while 1:\n",
    "        predictions = []\n",
    "        try:\n",
    "            for i in tqdm(range(0, len(images), batch_size)):\n",
    "                predictions.extend(detector.get_detections_for_batch(np.array(images[i:i + batch_size])))\n",
    "        except RuntimeError:\n",
    "            if batch_size == 1: \n",
    "                raise RuntimeError('Image too big to run face detection on GPU. Please use the --resize_factor argument')\n",
    "            batch_size //= 2\n",
    "            print('Recovering from OOM error; New batch size: {}'.format(batch_size))\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    results = []\n",
    "    pady1, pady2, padx1, padx2 = pads\n",
    "    for rect, image in zip(predictions, images):\n",
    "        if rect is None:\n",
    "            cv2.imwrite('temp/faulty_frame.jpg', image) # check this frame where the face was not detected.\n",
    "            raise ValueError('Face not detected! Ensure the video contains a face in all the frames.')\n",
    "\n",
    "        y1 = max(0, rect[1] - pady1)\n",
    "        y2 = min(image.shape[0], rect[3] + pady2)\n",
    "        x1 = max(0, rect[0] - padx1)\n",
    "        x2 = min(image.shape[1], rect[2] + padx2)\n",
    "\n",
    "        results.append([x1, y1, x2, y2])\n",
    "\n",
    "    boxes = np.array(results)\n",
    "    if not nosmooth: boxes = get_smoothened_boxes(boxes, T=5)\n",
    "    results = [[image[y1: y2, x1:x2], (y1, y2, x1, x2)] for image, (x1, y1, x2, y2) in zip(images, boxes)]\n",
    "\n",
    "    del detector\n",
    "    return results \n",
    "\n",
    "box=[-1,-1,-1,-1]\n",
    "\n",
    "def datagen(frames, mels):\n",
    "    img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n",
    "\n",
    "    if box[0] == -1:# 如果未指定 特定的人脸边界的话\n",
    "        if not static:# 是否使用视频的第一帧作为参考\n",
    "            face_det_results = face_detect(frames) # BGR2RGB for CNN face detection\n",
    "        else:\n",
    "            face_det_results = face_detect([frames[0]])\n",
    "    else:\n",
    "        print('Using the specified bounding box instead of face detection...')\n",
    "        y1, y2, x1, x2 = box\n",
    "        face_det_results = [[f[y1: y2, x1:x2], (y1, y2, x1, x2)] for f in frames] # 裁剪出人脸结果\n",
    "\n",
    "    for i, m in enumerate(mels):\n",
    "        idx = 0 if static else i%len(frames)\n",
    "        frame_to_save = frames[idx].copy()\n",
    "        face, coords = face_det_results[idx].copy()\n",
    "\n",
    "        face = cv2.resize(face, (img_size, img_size)) # 重采样到指定大小\n",
    "\n",
    "        img_batch.append(face)\n",
    "        mel_batch.append(m)\n",
    "        frame_batch.append(frame_to_save)\n",
    "        coords_batch.append(coords)\n",
    "\n",
    "        if len(img_batch) >= wav2lip_batch_size:\n",
    "            img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n",
    "\n",
    "            img_masked = img_batch.copy()\n",
    "            img_masked[:, img_size//2:] = 0\n",
    "\n",
    "            img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n",
    "            mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
    "\n",
    "            yield img_batch, mel_batch, frame_batch, coords_batch\n",
    "            img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n",
    "\n",
    "    if len(img_batch) > 0:\n",
    "        img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n",
    "\n",
    "        img_masked = img_batch.copy()\n",
    "        img_masked[:, img_size//2:] = 0\n",
    "\n",
    "        img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n",
    "        mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
    "\n",
    "        yield img_batch, mel_batch, frame_batch, coords_batch\n",
    "\n",
    "mel_step_size = 16 \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} for inference.'.format(device))\n",
    "\n",
    "\n",
    "#加载模型\n",
    "def _load(checkpoint_path):\n",
    "    if device == 'cuda':\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint\n",
    "\n",
    "def load_model(path):\n",
    "    model = Wav2Lip()\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    s = checkpoint[\"state_dict\"]\n",
    "    new_s = {}\n",
    "    for k, v in s.items():\n",
    "        new_s[k.replace('module.', '')] = v\n",
    "    model.load_state_dict(new_s)\n",
    "\n",
    "    model = model.to(device)\n",
    "    return model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fa28804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T23:27:40.890522Z",
     "iopub.status.busy": "2021-08-02T23:27:40.888088Z",
     "iopub.status.idle": "2021-08-02T23:27:40.891429Z",
     "shell.execute_reply": "2021-08-02T23:27:40.891984Z"
    },
    "papermill": {
     "duration": 23.580961,
     "end_time": "2021-08-02T23:27:40.892142",
     "exception": false,
     "start_time": "2021-08-02T23:27:17.311181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.mkdir('/kaggle/working/temp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57515b37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T23:28:28.923711Z",
     "iopub.status.busy": "2021-08-02T23:28:28.922966Z",
     "iopub.status.idle": "2021-08-02T23:28:47.888802Z",
     "shell.execute_reply": "2021-08-02T23:28:47.888202Z",
     "shell.execute_reply.started": "2021-07-24T14:00:13.510341Z"
    },
    "papermill": {
     "duration": 43.37012,
     "end_time": "2021-08-02T23:28:47.888949",
     "exception": false,
     "start_time": "2021-08-02T23:28:04.518829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▎        | 1/8 [00:04<00:28,  4.08s/it]\u001b[A\n",
      " 25%|██▌       | 2/8 [00:05<00:15,  2.54s/it]\u001b[A\n",
      " 38%|███▊      | 3/8 [00:06<00:10,  2.03s/it]\u001b[A\n",
      " 50%|█████     | 4/8 [00:08<00:07,  1.80s/it]\u001b[A\n",
      " 62%|██████▎   | 5/8 [00:09<00:05,  1.68s/it]\u001b[A\n",
      " 75%|███████▌  | 6/8 [00:11<00:03,  1.60s/it]\u001b[A\n",
      " 88%|████████▊ | 7/8 [00:12<00:01,  1.59s/it]\u001b[A\n",
      "100%|██████████| 8/8 [00:14<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoint from: /kaggle/working/wav2lip_checkpoints/checkpoint_step000012000.pth\n",
      "Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:18<00:00, 18.96s/it]\n"
     ]
    }
   ],
   "source": [
    "full_frames = full_frames[:len(mel_chunks)]\n",
    "\n",
    "batch_size = wav2lip_batch_size\n",
    "gen = datagen(full_frames.copy(), mel_chunks)  # 进行人脸的裁剪与拼接，6通道\n",
    "\n",
    "for i, (img_batch, mel_batch, frames, coords) in enumerate(tqdm(gen, \n",
    "                                        total=int(np.ceil(float(len(mel_chunks))/batch_size)))):\n",
    "    #加载模型\n",
    "    if i == 0:\n",
    "        model = load_model(checkpoint_path)\n",
    "        print (\"Model loaded\")\n",
    "\n",
    "        frame_h, frame_w = full_frames[0].shape[:-1]\n",
    "        #暂存临时视频\n",
    "        out = cv2.VideoWriter('/kaggle/working/temp/result_without_audio.mp4',\n",
    "                                cv2.VideoWriter_fourcc(*'DIVX'), fps, (frame_w, frame_h))\n",
    "\n",
    "    img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)\n",
    "    mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)\n",
    "    \n",
    "    \n",
    "    ##### 将 img_batch, mel_batch送入模型得到pred\n",
    "    ##############TODO##############\n",
    "    with torch.no_grad():\n",
    "        pred = model(mel_batch, img_batch)\n",
    "    \n",
    "    pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\n",
    "\n",
    "    for p, f, c in zip(pred, frames, coords):\n",
    "        y1, y2, x1, x2 = c\n",
    "        p = cv2.resize(p.astype(np.uint8), (x2 - x1, y2 - y1))\n",
    "\n",
    "        f[y1:y2, x1:x2] = p\n",
    "        out.write(f)\n",
    "\n",
    "out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "261125e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T23:29:35.382084Z",
     "iopub.status.busy": "2021-08-02T23:29:35.381188Z",
     "iopub.status.idle": "2021-08-02T23:29:35.385242Z",
     "shell.execute_reply": "2021-08-02T23:29:35.384706Z"
    },
    "papermill": {
     "duration": 23.669641,
     "end_time": "2021-08-02T23:29:35.385406",
     "exception": false,
     "start_time": "2021-08-02T23:29:11.715765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.mkdir('/kaggle/working/result/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9505c790",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T23:30:22.884289Z",
     "iopub.status.busy": "2021-08-02T23:30:22.883588Z",
     "iopub.status.idle": "2021-08-02T23:30:25.914186Z",
     "shell.execute_reply": "2021-08-02T23:30:25.913649Z",
     "shell.execute_reply.started": "2021-07-24T14:01:22.990563Z"
    },
    "papermill": {
     "duration": 27.19117,
     "end_time": "2021-08-02T23:30:25.914323",
     "exception": false,
     "start_time": "2021-08-02T23:29:58.723153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将生成的视频与语音合并\n",
    "outfile=\"/kaggle/working/result/result.mp4\"# 最终输出结果到该文件夹下\n",
    "command = 'ffmpeg -y -i {} -i {} -strict -2 -q:v 1 {}'.format(speech, '/kaggle/working/temp/result_without_audio.mp4',outfile)\n",
    "subprocess.call(command, shell=platform.system() != 'Windows')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30478.041156,
   "end_time": "2021-08-02T23:30:53.753624",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-02T15:02:55.712468",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
