{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a35e39",
   "metadata": {
    "papermill": {
     "duration": 0.03953,
     "end_time": "2021-08-02T15:03:04.941630",
     "exception": false,
     "start_time": "2021-08-02T15:03:04.902100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Wav2Lip\n",
    "**[Wav2Lip](https://arxiv.org/pdf/2008.10010.pdf)** 是一种基于对抗生成网络的由语音驱动的人脸说话视频生成模型。如下图所示，Wav2Lip的网络模型总体上分成三块：生成器、判别器和一个预训练好的Lip-Sync Expert组成。网络的输入有2个：任意的一段视频和一段语音，输出为一段唇音同步的视频。生成器是基于encoder-decoder的网络结构，分别利用2个encoder: speech encoder, identity encoder去对输入的语音和视频人脸进行编码，并将二者的编码结果进行拼接，送入到 face decoder 中进行解码得到输出的视频帧。判别器Visual Quality Discriminator对生成结果的质量进行规范，提高生成视频的清晰度。为了更好的保证生成结果的唇音同步性，Wav2Lip引入了一个预预训练的唇音同步判别模型 Pre-trained Lip-sync Expert，作为衡量生成结果的唇音同步性的额外损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34513e2",
   "metadata": {
    "papermill": {
     "duration": 0.037914,
     "end_time": "2021-08-02T15:03:05.018513",
     "exception": false,
     "start_time": "2021-08-02T15:03:04.980599",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Lip-Sync Expert\n",
    "Lip-sync Expert基于 **[SyncNet](https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16a/)**，是一种用来判别语音和视频是否同步的网络模型。如下图所示，SyncNet的输入也是两种：语音特征MFCC和嘴唇的视频帧，利用两个基于卷积神经网络的Encoder分别对输入的语音和视频帧进行降纬和特征提取，将二者的特征都映射到同一个纬度空间中去，最后利用contrastive loss对唇音同步性进行衡量，结果的值越大代表越不同步，结果值越小则代表越同步。在Wav2Lip模型中，进一步改进了SyncNet的网络结构：网络更深；加入了残差网络结构；输入的语音特征被替换成了mel-spectrogram特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461cfed",
   "metadata": {
    "papermill": {
     "duration": 0.07419,
     "end_time": "2021-08-02T15:03:05.158517",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.084327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. 环境的配置\n",
    "- `建议准备一台有显卡的linux系统电脑，或者可以选择使用第三方云服务器（Google Colab）` \n",
    "- `Python 3.6 或者更高版本` \n",
    "- ffmpeg: `sudo apt-get install ffmpeg`\n",
    "- 必要的python包的安装，所需要的库名称都已经包含在`requirements.txt`文件中，可以使用 `pip install -r requirements.txt`一次性安装. \n",
    "- 在本实验中利用到了人脸检测的相关技术，需要下载人脸检测预训练模型：Face detection [pre-trained model](https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth) 并移动到 `face_detection/detection/sfd/s3fd.pth`文件夹下. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1514d8",
   "metadata": {
    "papermill": {
     "duration": 0.081877,
     "end_time": "2021-08-02T15:03:05.314858",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.232981",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c15ff",
   "metadata": {
    "papermill": {
     "duration": 0.065854,
     "end_time": "2021-08-02T15:03:05.446911",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.381057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. 数据集的准备及预处理\n",
    "\n",
    "**LRS2 数据集的下载**  \n",
    "实验所需要的数据集下载地址为：<a href=\"http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html\">LRS2 dataset</a>，下载该数据集需要获得BBC的许可，需要发送申请邮件以获取下载密钥，具体操作详见网页中的指示。下载完成后对数据集进行解压到本目录的`mvlrs_v1/`文件夹下，并将LRS2中的文件列表文件`train.txt, val.txt, test.txt` 移动到`filelists/`文件夹下，最终得到的数据集目录结构如下所示。\n",
    "```\n",
    "data_root (mvlrs_v1)\n",
    "├── main, pretrain (我们只使用main文件夹下的数据)\n",
    "|\t├── 文件夹列表\n",
    "|\t│   ├── 5位以.mp4结尾的视频ID\n",
    "```\n",
    "**数据集预处理**\n",
    "数据集中大多数视频都是包含人的半身或者全身的画面，而我们的模型只需要人脸这一小部分。所以在预处理阶段，我们要对每一个视频进行分帧操作，提取视频的每一帧，之后使用`face detection`工具包对人脸位置进行定位并裁减，只保留人脸的图片帧。同时，我们也需要将每一个视频中的语音分离出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b7991d-71f0-4cce-a371-3c35f97c191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931aaa3d-6eab-44f9-9cc0-d6e17fe80c26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-29 23:09:29--  https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\n",
      "Resolving www.adrianbulat.com (www.adrianbulat.com)... 45.136.29.207\n",
      "Connecting to www.adrianbulat.com (www.adrianbulat.com)|45.136.29.207|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 89843225 (86M) [application/octet-stream]\n",
      "Saving to: ‘face_detection/detection/sfd/s3fd.pth’\n",
      "\n",
      "100%[======================================>] 89,843,225  23.7MB/s   in 3.6s   \n",
      "\n",
      "2023-08-29 23:09:33 (23.7 MB/s) - ‘face_detection/detection/sfd/s3fd.pth’ saved [89843225/89843225]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" -O face_detection/detection/sfd/s3fd.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bccdb9-1b21-48d4-a983-6f2add90b315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf ../LSR2/demo\n",
    "!mkdir -p ../LSR2/demo\n",
    "!cp -r ../LSR2/main/553* ../LSR2/demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ed106-5da1-4eb6-8507-b43458022c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pythob generate_hq_videos.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f57fe-3314-4561-baf2-ba4a45ed261c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# import os\n",
    "# import shutil\n",
    "# from tqdm import tqdm\n",
    "# from multiprocessing import Pool\n",
    "\n",
    "# codeformer_cmd = 'cd ../CodeFormer && python inference_codeformer.py --bg_upsampler realesrgan --face_upsample -w 1.0 -s 1 --input_path {} --output_path {}'\n",
    "# base_dir = '../LSR2/demo'\n",
    "# sub_dirs = os.listdir(base_dir)\n",
    "\n",
    "# cmds = []\n",
    "\n",
    "# def execute_cmd(cmd):\n",
    "#     os.system(cmd[0])\n",
    "#     shutil.copy(cmd[1][0], cmd[1][1])\n",
    "\n",
    "# for sub_dir in sub_dirs:\n",
    "#     sub_dir = os.path.join(base_dir, sub_dir)\n",
    "#     filenames = os.listdir(sub_dir)\n",
    "#     for filename in tqdm(filenames):\n",
    "#         if filename.endswith('mp4') and '_hq' not in filename:\n",
    "#             full_filename = os.path.join(sub_dir, filename)\n",
    "#             new_filename = full_filename[:-4]+'_hq'+full_filename[-4:]\n",
    "#             new_dirname = full_filename[:-4]\n",
    "#             # print(codeformer_cmd.format(full_filename, new_dirname))\n",
    "#             # print(os.path.join(new_dirname, filename), new_filename)\n",
    "#             if not os.path.exists(new_filename):\n",
    "#                 cmds.append((codeformer_cmd.format(full_filename, new_dirname), (os.path.join(new_dirname, filename), new_filename)))\n",
    "#                 # os.system(codeformer_cmd.format(full_filename, new_dirname))\n",
    "#                 # shutil.copy(os.path.join(new_dirname, filename), new_filename)\n",
    "\n",
    "# if len(cmds)>0:\n",
    "#     print('cmds:', len(cmds), cmds[0])\n",
    "#     with Pool(4) as p:\n",
    "#         p.map(execute_cmd, tqdm(cmds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b53dcfde",
   "metadata": {
    "papermill": {
     "duration": 0.076313,
     "end_time": "2021-08-02T15:03:05.590493",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.514180",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ../LSR2/lrs2_preprocessed_288x288\n",
    "!python preprocess.py --data_root \"../LSR2/main_hq\" --preprocessed_root \"../LSR2/lrs2_preprocessed_288x288\" --batch_size 32 --ngpu 4\n",
    "# !rm -rf ../LSR2/lrs2_preprocessed_288x288-demo\n",
    "# !python preprocess.py --data_root \"../LSR2/demo\" --preprocessed_root \"../LSR2/lrs2_preprocessed_288x288-demo\" --batch_size 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e70f29",
   "metadata": {
    "papermill": {
     "duration": 0.057029,
     "end_time": "2021-08-02T15:03:05.717822",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.660793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "预处理后的`lrs2_preprocessed/`文件夹下的目录结构如下\n",
    "```\n",
    "preprocessed_root (lrs2_preprocessed)\n",
    "├── 文件夹列表\n",
    "|\t├── 五位的视频ID\n",
    "|\t│   ├── *.jpg\n",
    "|\t│   ├── audio.wav\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a91f05-71fd-47f1-9488-df60a5653f0e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# codeformer_cmd = 'cd ../CodeFormer && python inference_codeformer.py --bg_upsampler realesrgan --face_upsample -w 1.0 --input_path {} --output_path {}'\n",
    "# preprocessed_root = \"../LSR2/lrs2_preprocessed_288x288-demo\"\n",
    "# sub_dirs = os.listdir(preprocessed_root)\n",
    "# for sub_dir in tqdm(sub_dirs):\n",
    "#     video_dirs = os.listdir(os.path.join(preprocessed_root, sub_dir))\n",
    "#     for video_dir in video_dirs:\n",
    "#         video_dir = os.path.join(preprocessed_root, sub_dir, video_dir)\n",
    "#         # print(video_dir)\n",
    "#         # print(codeformer_cmd.format(video_dir, video_dir))\n",
    "#         os.system(codeformer_cmd.format(video_dir, video_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff5bb63-382e-4670-bd8e-aa6d7702c34d",
   "metadata": {},
   "source": [
    "获取对应的文件列表并更新到filelists/train.txt和filelists/eval.txt。只保存对应的视频名称即可。代码可以参考，对视频样本重命名并生成对应的命名列表，此处视频文件数量过少<2，会报错："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c52dd89-d180-4fac-900b-219cd4147b98",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# from glob import glob\n",
    "# import shutil,os\n",
    " \n",
    "# from sklearn.model_selection import train_test_split\n",
    " \n",
    "# preprocessed_root = \"../LSR2/lrs2_preprocessed_288x288-demo\"\n",
    "\n",
    "# # 去除名字的特殊符号，统一序号视频文件命名\n",
    " \n",
    "# # def original_video_name_format():\n",
    "# #     base_path = \"../LSR2/main\"\n",
    "# #     result = list(glob(\"{}/*\".format(base_path),recursive=False))\n",
    "# #     file_num = 0\n",
    "# #     result_list = []\n",
    " \n",
    "# #     for each in result:\n",
    "# #         file_num +=1\n",
    "# #         new_position =\"{0}{1}\".format( int(time.time()),file_num)\n",
    "# #         result_list.append(new_position)\n",
    "# #         shutil.move(each, os.path.join(base_path,new_position+\".mp4\"))\n",
    "# #         pass\n",
    "\n",
    "# def trained_data_name_format():\n",
    "#     base_path = preprocessed_root\n",
    "#     # result = list(glob(\"{}/*\".format(base_path)))\n",
    "#     result = os.listdir(base_path)\n",
    "#     print(result)\n",
    "#     result_list = []\n",
    "#     for i,dirpath in enumerate(result):\n",
    "#         # shutil.move(dirpath,\"{0}/{1}\".format(base_path,i))\n",
    "#         # result_list.append(str(i))\n",
    "#         # print('dirpath:', dirpath)\n",
    "#         result_list.append(dirpath)\n",
    "#     if len(result_list)<14:\n",
    "#         test_result=val_result=train_result=result_list\n",
    "#     else:\n",
    "#         train_result,test_result = train_test_split(result_list,test_size=0.15, random_state=42)\n",
    "#         test_result, val_result = train_test_split(test_result, test_size=0.5, random_state=42)\n",
    " \n",
    "#     for file_name,dataset in zip((\"train.txt\",\"test.txt\",\"val.txt\"),(train_result,test_result,val_result)):\n",
    "#         with open(os.path.join(\"filelists\",file_name),'w',encoding='utf-8') as fi:\n",
    "#             for dataset_i in dataset:\n",
    "#                 # print('dataset_i:', dataset_i)\n",
    "#                 video_result = os.listdir(os.path.join(base_path, dataset_i))\n",
    "#                 # print('video_result:', video_result)\n",
    "#                 video_result = [dataset_i+'/'+video for video in video_result]\n",
    "#                 fi.write(\"\\n\".join(video_result))\n",
    "#                 fi.write(\"\\n\")\n",
    " \n",
    "#     # print(\"\\n\".join(result_list))\n",
    "\n",
    "# trained_data_name_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7921c0-363d-41e9-9533-7ab4715f97b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate_filelists.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f6d2a7-7f3e-40ea-96c4-996aaee42a32",
   "metadata": {},
   "source": [
    "Training the expert discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0bb64-29af-486d-b14e-5eecf0be8c17",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python color_syncnet_train.py --data_root ../LSR2/lrs2_preprocessed_288x288/ --checkpoint_dir ./savedmodel --checkpoint_path ./checkpoints/lipsync_expert.pth\n",
    "!python color_syncnet_train.py --data_root ../LSR2/lrs2_preprocessed_288x288/ --checkpoint_dir ./savedmodel \n",
    "!python color_syncnet_train.py --data_root ../LSR2/lrs2_preprocessed_288x288/ --checkpoint_dir ./savedmodel --checkpoint_path ./savedmodel/checkpoint_step000032000.pth \n",
    "# --checkpoint_path ./checkpoints/lipsync_expert.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b3dc53-722f-428e-8305-7b9d42ae082a",
   "metadata": {},
   "source": [
    "执行如下命令，开始训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b989d6e-be14-47fc-9f10-2226f02bc4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python wav2lip_train.py --data_root ../LSR2/lrs2_preprocessed --checkpoint_dir ./savedmodel --syncnet_checkpoint_path ./checkpoints/lipsync_expert.pth \n",
    "!python wav2lip_train.py --data_root ../LSR2/lrs2_preprocessed_288x288 --checkpoint_dir ./savedmodel --syncnet_checkpoint_path ./savedmodel/checkpoint_step000032000.pth \n",
    "# --checkpoint_path ./checkpoints/wav2lip.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764fda7-3c76-45d8-be8f-391ad409a361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python hq_wav2lip_train.py --data_root ../LSR2/lrs2_preprocessed_288x288 --checkpoint_dir ./savedmodel --syncnet_checkpoint_path ./savedmodel/checkpoint_step000032000.pth \n",
    "# !python hq_wav2lip_train.py --data_root ../LSR2/lrs2_preprocessed --checkpoint_dir ./savedmodel --syncnet_checkpoint_path ./checkpoints/lipsync_expert.pth --checkpoint_path ./checkpoints/wav2lip.pth --disc_checkpoint_path ./checkpoints/visual_quality_disc.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04656c-3efc-4ddc-a4db-1f328b5cf457",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python wloss_hq_wav2lip_train.py --data_root ../LSR2/lrs2_preprocessed_288x288/ --checkpoint_dir ./savedmodel --syncnet_checkpoint_path ./checkpoints/lipsync_expert.pth\n",
    "!python wloss_hq_wav2lip_train.py --data_root ../LSR2/lrs2_preprocessed_288x288/ --checkpoint_dir ./savedmodel --syncnet_checkpoint_path ./savedmodel/checkpoint_step000050000.pth \n",
    "# --checkpoint_path ./checkpoints/wav2lip.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f8d8df-60a8-42e7-8827-b7eabf39cc42",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:2 for inference.\n",
      "Reading video frames...\n",
      "Number of frames available for inference: 380\n",
      "(80, 936)\n",
      "Length of mel chunks: 232\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/15 [00:05<?, ?it/s]\u001b[A\n",
      "Recovering from OOM error; New batch size: 8\n",
      "\n",
      "  0%|                                                    | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▌                                          | 1/29 [01:26<40:32, 86.89s/it]\u001b[A\n",
      "  7%|███                                         | 2/29 [01:28<16:27, 36.57s/it]\u001b[A\n",
      " 10%|████▌                                       | 3/29 [01:29<08:52, 20.49s/it]\u001b[A\n",
      " 14%|██████                                      | 4/29 [01:30<05:23, 12.93s/it]\u001b[A\n",
      " 17%|███████▌                                    | 5/29 [01:32<03:29,  8.75s/it]\u001b[A\n",
      " 21%|█████████                                   | 6/29 [01:33<02:23,  6.23s/it]\u001b[A\n",
      " 24%|██████████▌                                 | 7/29 [01:34<01:41,  4.63s/it]\u001b[A\n",
      " 28%|████████████▏                               | 8/29 [01:36<01:15,  3.58s/it]\u001b[A\n",
      " 31%|█████████████▋                              | 9/29 [01:37<00:57,  2.88s/it]\u001b[A\n",
      " 34%|██████████████▊                            | 10/29 [01:38<00:45,  2.40s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 11/29 [01:40<00:37,  2.08s/it]\u001b[A\n",
      " 41%|█████████████████▊                         | 12/29 [01:41<00:31,  1.88s/it]\u001b[A\n",
      " 45%|███████████████████▎                       | 13/29 [01:43<00:27,  1.74s/it]\u001b[A\n",
      " 48%|████████████████████▊                      | 14/29 [01:44<00:24,  1.65s/it]\u001b[A\n",
      " 52%|██████████████████████▏                    | 15/29 [01:46<00:22,  1.59s/it]\u001b[A\n",
      " 55%|███████████████████████▋                   | 16/29 [01:47<00:19,  1.54s/it]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 17/29 [01:48<00:17,  1.50s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 18/29 [01:50<00:16,  1.47s/it]\u001b[A\n",
      " 66%|████████████████████████████▏              | 19/29 [01:51<00:14,  1.45s/it]\u001b[A\n",
      " 69%|█████████████████████████████▋             | 20/29 [01:53<00:12,  1.44s/it]\u001b[A\n",
      " 72%|███████████████████████████████▏           | 21/29 [01:54<00:11,  1.43s/it]\u001b[A\n",
      " 76%|████████████████████████████████▌          | 22/29 [01:55<00:09,  1.42s/it]\u001b[A\n",
      " 79%|██████████████████████████████████         | 23/29 [01:57<00:08,  1.42s/it]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 24/29 [01:58<00:07,  1.41s/it]\u001b[A\n",
      " 86%|█████████████████████████████████████      | 25/29 [02:00<00:05,  1.41s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 26/29 [02:01<00:04,  1.41s/it]\u001b[A\n",
      " 93%|████████████████████████████████████████   | 27/29 [02:02<00:02,  1.41s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▌ | 28/29 [02:04<00:01,  1.41s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 29/29 [02:05<00:00,  4.33s/it]\u001b[A\n",
      "Load checkpoint from: ./savedmodel/wav2lip_checkpoint_step000001000.pth\n",
      "Model loaded\n",
      "100%|█████████████████████████████████████████████| 2/2 [02:53<00:00, 86.53s/it]\n",
      "ffmpeg version 4.2.3 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "  built with gcc 7.5.0 (crosstool-NG 1.24.0.123_1667d2b)\n",
      "  configuration: --prefix=/home/conda/feedstock_root/build_artifacts/ffmpeg_1590573566052/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1590573566052/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "\u001b[0;33mGuessed Channel Layout for Input Stream #0.0 : stereo\n",
      "\u001b[0mInput #0, wav, from '../videos/test.wav':\n",
      "  Metadata:\n",
      "    encoder         : Lavf60.3.100\n",
      "  Duration: 00:00:11.69, bitrate: 1536 kb/s\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, stereo, s16, 1536 kb/s\n",
      "Input #1, avi, from 'temp/result.avi':\n",
      "  Metadata:\n",
      "    encoder         : Lavf59.27.100\n",
      "  Duration: 00:00:11.60, start: 0.000000, bitrate: 2727 kb/s\n",
      "    Stream #1:0: Video: mpeg4 (Simple Profile) (DIVX / 0x58564944), yuv420p, 1080x1920 [SAR 1:1 DAR 9:16], 2730 kb/s, 20 fps, 20 tbr, 20 tbn, 20 tbc\n",
      "Stream mapping:\n",
      "  Stream #1:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
      "  Stream #0:0 -> #0:1 (pcm_s16le (native) -> aac (native))\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0m\u001b[0;33m-qscale is ignored, -crf is recommended.\n",
      "\u001b[0m\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0musing SAR=1/1\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0mprofile High, level 4.0\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0m264 - core 152 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=48 lookahead_threads=8 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=20 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'results/result_voice.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p(progressive), 1080x1920 [SAR 1:1 DAR 9:16], q=-1--1, 20 fps, 10240 tbn, 20 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "    Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 128 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 aac\n",
      "frame=  232 fps= 88 q=-1.0 Lsize=    3093kB time=00:00:11.69 bitrate=2167.2kbits/s speed=4.44x    \n",
      "video:2897kB audio:186kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.301477%\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0mframe I:3     Avg QP:17.28  size: 42952\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0mframe P:149   Avg QP:19.61  size: 16241\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0mframe B:80    Avg QP:22.65  size:  5219\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0mconsecutive B-frames: 40.9% 34.5% 14.2% 10.3%\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0mmb I  I16..4: 38.1% 61.0%  1.0%\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0mmb P  I16..4:  3.7% 11.1%  0.0%  P16..4: 33.9%  4.6%  2.0%  0.0%  0.0%    skip:44.7%\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0mmb B  I16..4:  0.3%  1.1%  0.0%  B16..8: 30.4%  0.9%  0.0%  direct: 0.4%  skip:66.9%  L0:69.3% L1:28.5% BI: 2.2%\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0m8x8 transform intra:73.4% inter:95.4%\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0mcoded y,uvDC,uvAC intra: 48.9% 52.2% 7.9% inter: 7.2% 10.2% 0.1%\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0mi16 v,h,dc,p: 32% 46% 12%  9%\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 31% 31% 32%  1%  1%  1%  1%  1%  1%\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 52% 35%  7%  1%  1%  2%  1%  1%  1%\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0mi8c dc,h,v,p: 41% 26% 27%  6%\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0mref P L0: 73.6%  5.4% 15.3%  5.7%\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0mref B L0: 77.2% 21.0%  1.8%\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0mref B L1: 97.9%  2.1%\n",
      "\u001b[1;36m[libx264 @ 0x55e305535080] \u001b[0mkb/s:2045.70\n",
      "\u001b[1;36m[aac @ 0x55e305541e40] \u001b[0mQavg: 1004.547\n",
      "\u001b[0mCPU times: user 1.87 s, sys: 745 ms, total: 2.62 s\n",
      "Wall time: 3min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python inference.py --checkpoint_path ./savedmodel/wav2lip_checkpoint_step000093000.pth --face ../videos/97.mp4 --audio ../videos/test.wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89403387-1a29-417f-87c2-1b98e48a902e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python inference.py --checkpoint_path ./savedmodel/wav2lip_checkpoint_step000008000.pth --face ../LSR2/demo/5539702505926936192/00001.mp4 --audio ../LSR2/lrs2_preprocessed_288x288-demo/5539702505926936192/00001/audio.wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93027a98-712e-4447-89d3-6793ad643e12",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !cd ../LSR2/lrs2_preprocessed_288x288-demo/5539702505926936192/00001/final_results/ && ffmpeg -r 25 -i %d.png 00001-sr.mp4 -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da52ca1-f367-4c4b-84a5-81e5a968a6fb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python inference.py --checkpoint_path ./savedmodel/wav2lip_checkpoint_step000008000.pth --face ../LSR2/lrs2_preprocessed_288x288-demo/5539702505926936192/00001/final_results/00001-sr.mp4 --audio ../LSR2/lrs2_preprocessed_288x288-demo/5539702505926936192/00001/audio.wav\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30478.041156,
   "end_time": "2021-08-02T23:30:53.753624",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-02T15:02:55.712468",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
