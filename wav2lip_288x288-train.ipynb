{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a35e39",
   "metadata": {
    "papermill": {
     "duration": 0.03953,
     "end_time": "2021-08-02T15:03:04.941630",
     "exception": false,
     "start_time": "2021-08-02T15:03:04.902100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Wav2Lip\n",
    "**[Wav2Lip](https://arxiv.org/pdf/2008.10010.pdf)** 是一种基于对抗生成网络的由语音驱动的人脸说话视频生成模型。如下图所示，Wav2Lip的网络模型总体上分成三块：生成器、判别器和一个预训练好的Lip-Sync Expert组成。网络的输入有2个：任意的一段视频和一段语音，输出为一段唇音同步的视频。生成器是基于encoder-decoder的网络结构，分别利用2个encoder: speech encoder, identity encoder去对输入的语音和视频人脸进行编码，并将二者的编码结果进行拼接，送入到 face decoder 中进行解码得到输出的视频帧。判别器Visual Quality Discriminator对生成结果的质量进行规范，提高生成视频的清晰度。为了更好的保证生成结果的唇音同步性，Wav2Lip引入了一个预预训练的唇音同步判别模型 Pre-trained Lip-sync Expert，作为衡量生成结果的唇音同步性的额外损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34513e2",
   "metadata": {
    "papermill": {
     "duration": 0.037914,
     "end_time": "2021-08-02T15:03:05.018513",
     "exception": false,
     "start_time": "2021-08-02T15:03:04.980599",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Lip-Sync Expert\n",
    "Lip-sync Expert基于 **[SyncNet](https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16a/)**，是一种用来判别语音和视频是否同步的网络模型。如下图所示，SyncNet的输入也是两种：语音特征MFCC和嘴唇的视频帧，利用两个基于卷积神经网络的Encoder分别对输入的语音和视频帧进行降纬和特征提取，将二者的特征都映射到同一个纬度空间中去，最后利用contrastive loss对唇音同步性进行衡量，结果的值越大代表越不同步，结果值越小则代表越同步。在Wav2Lip模型中，进一步改进了SyncNet的网络结构：网络更深；加入了残差网络结构；输入的语音特征被替换成了mel-spectrogram特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461cfed",
   "metadata": {
    "papermill": {
     "duration": 0.07419,
     "end_time": "2021-08-02T15:03:05.158517",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.084327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. 环境的配置\n",
    "- `建议准备一台有显卡的linux系统电脑，或者可以选择使用第三方云服务器（Google Colab）` \n",
    "- `Python 3.6 或者更高版本` \n",
    "- ffmpeg: `sudo apt-get install ffmpeg`\n",
    "- 必要的python包的安装，所需要的库名称都已经包含在`requirements.txt`文件中，可以使用 `pip install -r requirements.txt`一次性安装. \n",
    "- 在本实验中利用到了人脸检测的相关技术，需要下载人脸检测预训练模型：Face detection [pre-trained model](https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth) 并移动到 `face_detection/detection/sfd/s3fd.pth`文件夹下. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1514d8",
   "metadata": {
    "papermill": {
     "duration": 0.081877,
     "end_time": "2021-08-02T15:03:05.314858",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.232981",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c15ff",
   "metadata": {
    "papermill": {
     "duration": 0.065854,
     "end_time": "2021-08-02T15:03:05.446911",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.381057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. 数据集的准备及预处理\n",
    "\n",
    "**LRS2 数据集的下载**  \n",
    "实验所需要的数据集下载地址为：<a href=\"http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html\">LRS2 dataset</a>，下载该数据集需要获得BBC的许可，需要发送申请邮件以获取下载密钥，具体操作详见网页中的指示。下载完成后对数据集进行解压到本目录的`mvlrs_v1/`文件夹下，并将LRS2中的文件列表文件`train.txt, val.txt, test.txt` 移动到`filelists/`文件夹下，最终得到的数据集目录结构如下所示。\n",
    "```\n",
    "data_root (mvlrs_v1)\n",
    "├── main, pretrain (我们只使用main文件夹下的数据)\n",
    "|\t├── 文件夹列表\n",
    "|\t│   ├── 5位以.mp4结尾的视频ID\n",
    "```\n",
    "**数据集预处理**\n",
    "数据集中大多数视频都是包含人的半身或者全身的画面，而我们的模型只需要人脸这一小部分。所以在预处理阶段，我们要对每一个视频进行分帧操作，提取视频的每一帧，之后使用`face detection`工具包对人脸位置进行定位并裁减，只保留人脸的图片帧。同时，我们也需要将每一个视频中的语音分离出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931aaa3d-6eab-44f9-9cc0-d6e17fe80c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" -O face_detection/detection/sfd/s3fd.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bccdb9-1b21-48d4-a983-6f2add90b315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf ../LSR2/demo\n",
    "!mkdir -p ../LSR2/demo\n",
    "!cp -r ../LSR2/main/553* ../LSR2/demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f57fe-3314-4561-baf2-ba4a45ed261c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "codeformer_cmd = 'cd ../CodeFormer && python inference_codeformer.py --bg_upsampler realesrgan --face_upsample -w 1.0 -s 1 --input_path {} --output_path {}'\n",
    "base_dir = '../LSR2/demo'\n",
    "sub_dirs = os.listdir(base_dir)\n",
    "\n",
    "cmds = []\n",
    "\n",
    "def execute_cmd(cmd):\n",
    "    os.system(cmd[0])\n",
    "    shutil.copy(cmd[1][0], cmd[1][1])\n",
    "\n",
    "for sub_dir in sub_dirs:\n",
    "    sub_dir = os.path.join(base_dir, sub_dir)\n",
    "    filenames = os.listdir(sub_dir)\n",
    "    for filename in tqdm(filenames):\n",
    "        if filename.endswith('mp4') and '_hq' not in filename:\n",
    "            full_filename = os.path.join(sub_dir, filename)\n",
    "            new_filename = full_filename[:-4]+'_hq'+full_filename[-4:]\n",
    "            new_dirname = full_filename[:-4]\n",
    "            # print(codeformer_cmd.format(full_filename, new_dirname))\n",
    "            # print(os.path.join(new_dirname, filename), new_filename)\n",
    "            if not os.path.exists(new_filename):\n",
    "                cmds.append((codeformer_cmd.format(full_filename, new_dirname), (os.path.join(new_dirname, filename), new_filename)))\n",
    "                # os.system(codeformer_cmd.format(full_filename, new_dirname))\n",
    "                # shutil.copy(os.path.join(new_dirname, filename), new_filename)\n",
    "\n",
    "if len(cmds)>0:\n",
    "    print('cmds:', len(cmds), cmds[0])\n",
    "    with Pool(4) as p:\n",
    "        p.map(execute_cmd, tqdm(cmds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53dcfde",
   "metadata": {
    "papermill": {
     "duration": 0.076313,
     "end_time": "2021-08-02T15:03:05.590493",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.514180",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -rf ../LSR2/lrs2_preprocessed_288x288\n",
    "# !python preprocess.py --data_root \"../LSR2/main\" --preprocessed_root \"../LSR2/lrs2_preprocessed_288x288\" --batch_size 128\n",
    "!rm -rf ../LSR2/lrs2_preprocessed_288x288-demo\n",
    "!python preprocess.py --data_root \"../LSR2/demo\" --preprocessed_root \"../LSR2/lrs2_preprocessed_288x288-demo\" --batch_size 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e70f29",
   "metadata": {
    "papermill": {
     "duration": 0.057029,
     "end_time": "2021-08-02T15:03:05.717822",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.660793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "预处理后的`lrs2_preprocessed/`文件夹下的目录结构如下\n",
    "```\n",
    "preprocessed_root (lrs2_preprocessed)\n",
    "├── 文件夹列表\n",
    "|\t├── 五位的视频ID\n",
    "|\t│   ├── *.jpg\n",
    "|\t│   ├── audio.wav\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a91f05-71fd-47f1-9488-df60a5653f0e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# codeformer_cmd = 'cd ../CodeFormer && python inference_codeformer.py --bg_upsampler realesrgan --face_upsample -w 1.0 --input_path {} --output_path {}'\n",
    "# preprocessed_root = \"../LSR2/lrs2_preprocessed_288x288-demo\"\n",
    "# sub_dirs = os.listdir(preprocessed_root)\n",
    "# for sub_dir in tqdm(sub_dirs):\n",
    "#     video_dirs = os.listdir(os.path.join(preprocessed_root, sub_dir))\n",
    "#     for video_dir in video_dirs:\n",
    "#         video_dir = os.path.join(preprocessed_root, sub_dir, video_dir)\n",
    "#         # print(video_dir)\n",
    "#         # print(codeformer_cmd.format(video_dir, video_dir))\n",
    "#         os.system(codeformer_cmd.format(video_dir, video_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff5bb63-382e-4670-bd8e-aa6d7702c34d",
   "metadata": {},
   "source": [
    "获取对应的文件列表并更新到filelists/train.txt和filelists/eval.txt。只保存对应的视频名称即可。代码可以参考，对视频样本重命名并生成对应的命名列表，此处视频文件数量过少<2，会报错："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c52dd89-d180-4fac-900b-219cd4147b98",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from glob import glob\n",
    "import shutil,os\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "preprocessed_root = \"../LSR2/lrs2_preprocessed_288x288-demo\"\n",
    "\n",
    "# 去除名字的特殊符号，统一序号视频文件命名\n",
    " \n",
    "# def original_video_name_format():\n",
    "#     base_path = \"../LSR2/main\"\n",
    "#     result = list(glob(\"{}/*\".format(base_path),recursive=False))\n",
    "#     file_num = 0\n",
    "#     result_list = []\n",
    " \n",
    "#     for each in result:\n",
    "#         file_num +=1\n",
    "#         new_position =\"{0}{1}\".format( int(time.time()),file_num)\n",
    "#         result_list.append(new_position)\n",
    "#         shutil.move(each, os.path.join(base_path,new_position+\".mp4\"))\n",
    "#         pass\n",
    "\n",
    "def trained_data_name_format():\n",
    "    base_path = preprocessed_root\n",
    "    # result = list(glob(\"{}/*\".format(base_path)))\n",
    "    result = os.listdir(base_path)\n",
    "    print(result)\n",
    "    result_list = []\n",
    "    for i,dirpath in enumerate(result):\n",
    "        # shutil.move(dirpath,\"{0}/{1}\".format(base_path,i))\n",
    "        # result_list.append(str(i))\n",
    "        # print('dirpath:', dirpath)\n",
    "        result_list.append(dirpath)\n",
    "    if len(result_list)<14:\n",
    "        test_result=val_result=train_result=result_list\n",
    "    else:\n",
    "        train_result,test_result = train_test_split(result_list,test_size=0.15, random_state=42)\n",
    "        test_result, val_result = train_test_split(test_result, test_size=0.5, random_state=42)\n",
    " \n",
    "    for file_name,dataset in zip((\"train.txt\",\"test.txt\",\"val.txt\"),(train_result,test_result,val_result)):\n",
    "        with open(os.path.join(\"filelists\",file_name),'w',encoding='utf-8') as fi:\n",
    "            for dataset_i in dataset:\n",
    "                # print('dataset_i:', dataset_i)\n",
    "                video_result = os.listdir(os.path.join(base_path, dataset_i))\n",
    "                # print('video_result:', video_result)\n",
    "                video_result = [dataset_i+'/'+video for video in video_result]\n",
    "                fi.write(\"\\n\".join(video_result))\n",
    "                fi.write(\"\\n\")\n",
    " \n",
    "    # print(\"\\n\".join(result_list))\n",
    "\n",
    "trained_data_name_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f6d2a7-7f3e-40ea-96c4-996aaee42a32",
   "metadata": {},
   "source": [
    "Training the expert discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0bb64-29af-486d-b14e-5eecf0be8c17",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python color_syncnet_train.py --data_root ../LSR2/lrs2_preprocessed_288x288-demo/ --checkpoint_dir ./savedmodel \n",
    "# --checkpoint_path ./checkpoints/lipsync_expert.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b3dc53-722f-428e-8305-7b9d42ae082a",
   "metadata": {},
   "source": [
    "执行如下命令，开始训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b989d6e-be14-47fc-9f10-2226f02bc4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python wav2lip_train.py --data_root ../LSR2/lrs2_preprocessed --checkpoint_dir ./savedmodel --syncnet_checkpoint_path ./checkpoints/lipsync_expert.pth --checkpoint_path ./checkpoints/wav2lip.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764fda7-3c76-45d8-be8f-391ad409a361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python hq_wav2lip_train.py --data_root ../LSR2/lrs2_preprocessed --checkpoint_dir ./savedmodel --syncnet_checkpoint_path ./checkpoints/lipsync_expert.pth --checkpoint_path ./checkpoints/wav2lip.pth --disc_checkpoint_path ./checkpoints/visual_quality_disc.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04656c-3efc-4ddc-a4db-1f328b5cf457",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python wloss_hq_wav2lip_train.py --data_root ../LSR2/lrs2_preprocessed_288x288-demo/ --checkpoint_dir ./savedmodel --syncnet_checkpoint_path ./savedmodel_old2/checkpoint_step000007000.pth \n",
    "# --checkpoint_path ./checkpoints/wav2lip.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8d8df-60a8-42e7-8827-b7eabf39cc42",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!python inference.py --checkpoint_path ./savedmodel_old2/wav2lip_checkpoint_step000008000.pth --face ../VID20230620105517.mp4 --audio ../test.wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89403387-1a29-417f-87c2-1b98e48a902e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python inference.py --checkpoint_path ./savedmodel/wav2lip_checkpoint_step000008000.pth --face ../LSR2/demo/5539702505926936192/00001.mp4 --audio ../LSR2/lrs2_preprocessed_288x288-demo/5539702505926936192/00001/audio.wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93027a98-712e-4447-89d3-6793ad643e12",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !cd ../LSR2/lrs2_preprocessed_288x288-demo/5539702505926936192/00001/final_results/ && ffmpeg -r 25 -i %d.png 00001-sr.mp4 -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da52ca1-f367-4c4b-84a5-81e5a968a6fb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python inference.py --checkpoint_path ./savedmodel/wav2lip_checkpoint_step000008000.pth --face ../LSR2/lrs2_preprocessed_288x288-demo/5539702505926936192/00001/final_results/00001-sr.mp4 --audio ../LSR2/lrs2_preprocessed_288x288-demo/5539702505926936192/00001/audio.wav\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30478.041156,
   "end_time": "2021-08-02T23:30:53.753624",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-02T15:02:55.712468",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
